{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UhOrGWWiFg1F",
        "B0lq97BcKad8",
        "WTO1fyAOFcbz",
        "1VUd20iq-db_",
        "io03dPX7FY2S",
        "OxA-oc5EFSzp",
        "uNaMyDyyXbjl",
        "Gj3CEftWMgFF",
        "vNYbw3IP6lQ_",
        "12FU0cgIoDNS",
        "Klv7ze7rWE2l"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Suicidal Ideation Pt 1"
      ],
      "metadata": {
        "id": "xXyyZaG3FnR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package Installation"
      ],
      "metadata": {
        "id": "tztf_U9qFkjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8VdU2b139TX"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install optuna\n",
        "!pip install umap-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import random\n",
        "from torch.optim import Adam\n",
        "from torch_geometric.utils import negative_sampling, train_test_split_edges\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import umap\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "Xnt470v3-8_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "UhOrGWWiFg1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/features_final.csv'\n",
        "data_features = pd.read_csv(csv_file_path)\n",
        "data_features\n",
        "data_features_2 = data_features.copy()\n",
        "data_features_2"
      ],
      "metadata": {
        "id": "hV4KQvrQ-7wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_features.index = data_features.index.astype(int)\n",
        "new_index = pd.RangeIndex(start=0, stop=251, step=1)\n",
        "df_reindexed = data_features.reindex(new_index, fill_value=0)\n",
        "bool_columns = data_features.select_dtypes(include=bool).columns\n",
        "df_reindexed[bool_columns] = df_reindexed[bool_columns].fillna(False)\n",
        "df_reindexed\n",
        "x = torch.tensor(df_reindexed.values.astype(np.float32))\n",
        "x"
      ],
      "metadata": {
        "id": "ChBEGcxp_Dvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/confident_edgelist2.csv')\n",
        "edge_index = torch.tensor([df['pid'], df['alter']], dtype=torch.long)\n",
        "print(edge_index)"
      ],
      "metadata": {
        "id": "RejVaEtK_F8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.from_pandas_edgelist(df, source='pid', target='alter', create_using=nx.DiGraph)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "nx.draw(G, with_labels=True, node_size=300, node_color='lightblue', font_size=8)\n",
        "plt.title('Directed Graph')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xfQJOfZd_Iuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(df_reindexed.values.astype(np.float32))\n",
        "edge_index = torch.tensor([df['pid'], df['alter']], dtype=torch.long)"
      ],
      "metadata": {
        "id": "cEYrnRkH_KbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree (Accuracy Baseline)"
      ],
      "metadata": {
        "id": "B0lq97BcKad8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import pandas as pd\n",
        "\n",
        "def prepare_data_with_node_mapping(x, edge_index):\n",
        "    pos_edge_index = edge_index.T\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=torch.tensor(edge_index),\n",
        "        num_nodes=x.shape[0],\n",
        "        num_neg_samples=pos_edge_index.shape[0]\n",
        "    ).T\n",
        "\n",
        "    all_edges = np.concatenate([pos_edge_index, neg_edge_index], axis=0)\n",
        "    labels = np.concatenate([np.ones(pos_edge_index.shape[0]), np.zeros(neg_edge_index.shape[0])])\n",
        "\n",
        "    features = np.hstack([x[all_edges[:, 0]], x[all_edges[:, 1]]])\n",
        "\n",
        "    # Create a mapping of feature indices to nodes\n",
        "    node_mapping = []\n",
        "    for edge in all_edges:\n",
        "        node_mapping.append((edge[0], edge[1]))\n",
        "\n",
        "    return features, labels, node_mapping\n",
        "\n",
        "def test_decision_tree(train_features, train_labels, test_features, test_labels, feature_names=None, node_mapping=None):\n",
        "    clf = DecisionTreeClassifier(random_state=42)\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    test_preds = clf.predict(test_features)\n",
        "    test_probs = clf.predict_proba(test_features)\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, test_preds)\n",
        "    precision = precision_score(test_labels, test_preds)\n",
        "    recall = recall_score(test_labels, test_preds)\n",
        "    f1 = f1_score(test_labels, test_preds)\n",
        "    auc = roc_auc_score(test_labels, test_probs[:, 1])\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = clf.feature_importances_\n",
        "\n",
        "    # Sort features by importance score\n",
        "    importance_indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Filter features with importance greater than 0\n",
        "    relevant_indices = [i for i in importance_indices if importances[i] > 0]\n",
        "\n",
        "    # Get top features and their importance scores\n",
        "    top_feature_indices = relevant_indices[:10]  # Adjust as needed\n",
        "    top_feature_importances = importances[top_feature_indices]\n",
        "\n",
        "    if feature_names is not None:\n",
        "        # Ensure the feature_names list matches the number of features\n",
        "        feature_names = feature_names[:len(train_features[0])]\n",
        "        top_feature_names = [feature_names[i] for i in top_feature_indices if i < len(feature_names)]\n",
        "    else:\n",
        "        top_feature_names = [f'Feature_{i}' for i in top_feature_indices]  # Default names if feature_names is None\n",
        "\n",
        "    # Create node information if node_mapping is provided\n",
        "    if node_mapping is not None:\n",
        "        top_feature_node_mapping = [(node_mapping[i][0], node_mapping[i][1]) for i in top_feature_indices if i < len(node_mapping)]\n",
        "    else:\n",
        "        top_feature_node_mapping = None\n",
        "\n",
        "    # Export decision tree rules\n",
        "    try:\n",
        "        tree_rules = export_text(clf, feature_names=feature_names)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error exporting tree rules: {e}\")\n",
        "        tree_rules = None\n",
        "\n",
        "    return auc, accuracy, precision, recall, f1, top_feature_names, top_feature_importances, top_feature_node_mapping, tree_rules\n",
        "\n",
        "# Test the function\n",
        "num_splits = 10\n",
        "total_auc, total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0, 0\n",
        "all_top_feature_names = []\n",
        "all_top_feature_importances = []\n",
        "all_top_feature_node_mapping = []\n",
        "all_tree_rules = []\n",
        "\n",
        "for _ in range(num_splits):\n",
        "    data_random_split = Data(x=x, edge_index=edge_index)\n",
        "    split = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
        "    train_data, val_data, test_data = split(data_random_split)\n",
        "\n",
        "    # Prepare features and labels for training and testing\n",
        "    train_features, train_labels, train_node_mapping = prepare_data_with_node_mapping(train_data.x.numpy(), train_data.edge_index.numpy())\n",
        "    test_features, test_labels, test_node_mapping = prepare_data_with_node_mapping(test_data.x.numpy(), test_data.edge_index.numpy())\n",
        "\n",
        "    # Assuming feature_names is the column names from data_features_2\n",
        "    feature_names = data_features_2.columns.tolist()  # Ensure this matches your data\n",
        "\n",
        "    # Evaluate the decision tree\n",
        "    auc, accuracy, precision, recall, f1, top_feature_names, top_feature_importances, top_feature_node_mapping, tree_rules = test_decision_tree(\n",
        "        train_features, train_labels, test_features, test_labels, feature_names, test_node_mapping\n",
        "    )\n",
        "    total_auc += auc\n",
        "    total_accuracy += accuracy\n",
        "    total_precision += precision\n",
        "    total_recall += recall\n",
        "    total_f1 += f1\n",
        "\n",
        "    if top_feature_names is not None:\n",
        "        all_top_feature_names.extend(top_feature_names)\n",
        "        all_top_feature_importances.extend(top_feature_importances)\n",
        "        all_top_feature_node_mapping.extend(top_feature_node_mapping)\n",
        "\n",
        "    if tree_rules is not None:\n",
        "        all_tree_rules.append(tree_rules)\n",
        "\n",
        "# Calculate average metrics\n",
        "average_auc = total_auc / num_splits\n",
        "average_accuracy = total_accuracy / num_splits\n",
        "average_precision = total_precision / num_splits\n",
        "average_recall = total_recall / num_splits\n",
        "average_f1 = total_f1 / num_splits\n",
        "\n",
        "# Get unique top features and their total importance scores\n",
        "top_feature_names, top_feature_importances = zip(*sorted(zip(all_top_feature_names, all_top_feature_importances), key=lambda x: x[1], reverse=True))\n",
        "top_feature_names = list(top_feature_names)\n",
        "top_feature_importances = list(top_feature_importances)\n",
        "\n",
        "# Keep only top 10 features with non-zero importance\n",
        "top_features = list(zip(top_feature_names, top_feature_importances))\n",
        "top_features = [f for f in top_features if f[1] > 0][:10]\n",
        "top_feature_names, top_feature_importances = zip(*top_features) if top_features else ([], [])\n",
        "\n",
        "print(f'Average Metrics on Random Splits of Test Graphs - AUC: {average_auc:.4f}, Accuracy: {average_accuracy:.4f}, Precision: {average_precision:.4f}, Recall: {average_recall:.4f}, F1 Score: {average_f1:.4f}')\n",
        "\n",
        "# Display top features with associated node pairs\n",
        "for name, importance in zip(top_feature_names, top_feature_importances):\n",
        "    # Find all indices related to the feature name\n",
        "    related_node_mappings = []\n",
        "    for idx, top_name in enumerate(all_top_feature_names):\n",
        "        if top_name == name:\n",
        "            related_node_mappings.append(all_top_feature_node_mapping[idx])\n",
        "\n",
        "    print(f'Top Feature - Name: {name}, Importance: {importance}')\n",
        "    printed_count = 0\n",
        "    for mapping in related_node_mappings:\n",
        "        if printed_count >= 3:  # Print only 1-3 node pairs\n",
        "            break\n",
        "\n",
        "        source_node, dest_node = mapping\n",
        "\n",
        "        # Find feature values in the original dataframe (data_features_2)\n",
        "        source_features = data_features_2.iloc[source_node].to_dict()\n",
        "        dest_features = data_features_2.iloc[dest_node].to_dict()\n",
        "\n",
        "        # Filter attributes with True values\n",
        "        source_true_features = {k: v for k, v in source_features.items() if v is True}\n",
        "        dest_true_features = {k: v for k, v in dest_features.items() if v is True}\n",
        "\n",
        "        print(f'   Node Pair: ({source_node}, {dest_node})')\n",
        "        print(f'     Source Node True Feature Values:')\n",
        "        if source_true_features:\n",
        "            print(pd.DataFrame(list(source_true_features.items()), columns=['Feature', 'Value']).to_string(index=False))\n",
        "        else:\n",
        "            print('     No True features in source node.')\n",
        "\n",
        "        print(f'     Destination Node True Feature Values:')\n",
        "        if dest_true_features:\n",
        "            print(pd.DataFrame(list(dest_true_features.items()), columns=['Feature', 'Value']).to_string(index=False))\n",
        "        else:\n",
        "            print('     No True features in destination node.')\n",
        "\n",
        "        # Indicate which node the important feature came from\n",
        "        if name in source_true_features:\n",
        "            print(f'     Important feature came from source node.')\n",
        "        if name in dest_true_features:\n",
        "            print(f'     Important feature came from destination node.')\n",
        "\n",
        "        printed_count += 1\n",
        "\n",
        "# Print decision tree rules for each split\n",
        "for i, rules in enumerate(all_tree_rules):\n",
        "    print(f'\\nDecision Tree Rules for Split {i+1}:')\n",
        "    print(rules)\n"
      ],
      "metadata": {
        "id": "ga1o6H0HJfb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP baseline"
      ],
      "metadata": {
        "id": "6Zxc2nWudMh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ Prepare Data Function\n",
        "def prepare_data_with_node_mapping(x, edge_index):\n",
        "    pos_edge_index = edge_index.T\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=torch.tensor(edge_index),\n",
        "        num_nodes=x.shape[0],\n",
        "        num_neg_samples=pos_edge_index.shape[0]\n",
        "    ).T\n",
        "\n",
        "    all_edges = np.concatenate([pos_edge_index, neg_edge_index], axis=0)\n",
        "    labels = np.concatenate([np.ones(pos_edge_index.shape[0]), np.zeros(neg_edge_index.shape[0])])\n",
        "\n",
        "    features = np.hstack([x[all_edges[:, 0]], x[all_edges[:, 1]]])\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# ✅ MLP Training & Evaluation Function\n",
        "def test_mlp(train_features, train_labels, test_features, test_labels):\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', random_state=42, max_iter=200)\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    test_preds = clf.predict(test_features)\n",
        "    test_probs = clf.predict_proba(test_features)[:, 1]  # Get probabilities for AUC calculation\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, test_preds)\n",
        "    precision = precision_score(test_labels, test_preds)\n",
        "    recall = recall_score(test_labels, test_preds)\n",
        "    f1 = f1_score(test_labels, test_preds)\n",
        "    auc = roc_auc_score(test_labels, test_probs)\n",
        "\n",
        "    return auc, accuracy, precision, recall, f1\n",
        "\n",
        "# ✅ Run Multiple Splits & Compute Average Metrics\n",
        "num_splits = 10\n",
        "total_auc, total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0, 0\n",
        "\n",
        "for _ in range(num_splits):\n",
        "    data_random_split = Data(x=x, edge_index=edge_index)\n",
        "    split = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
        "    train_data, val_data, test_data = split(data_random_split)\n",
        "\n",
        "    # Prepare features and labels for training and testing\n",
        "    train_features, train_labels = prepare_data_with_node_mapping(train_data.x.numpy(), train_data.edge_index.numpy())\n",
        "    test_features, test_labels = prepare_data_with_node_mapping(test_data.x.numpy(), test_data.edge_index.numpy())\n",
        "\n",
        "    # Train & Evaluate MLP\n",
        "    auc, accuracy, precision, recall, f1 = test_mlp(train_features, train_labels, test_features, test_labels)\n",
        "\n",
        "    # Aggregate Metrics\n",
        "    total_auc += auc\n",
        "    total_accuracy += accuracy\n",
        "    total_precision += precision\n",
        "    total_recall += recall\n",
        "    total_f1 += f1\n",
        "\n",
        "# ✅ Compute Averages Across Splits\n",
        "average_auc = total_auc / num_splits\n",
        "average_accuracy = total_accuracy / num_splits\n",
        "average_precision = total_precision / num_splits\n",
        "average_recall = total_recall / num_splits\n",
        "average_f1 = total_f1 / num_splits\n",
        "\n",
        "# ✅ Print Results\n",
        "print(f'Average Metrics Over {num_splits} Splits:')\n",
        "print(f'AUC: {average_auc:.4f}')\n",
        "print(f'Accuracy: {average_accuracy:.4f}')\n",
        "print(f'Precision: {average_precision:.4f}')\n",
        "print(f'Recall: {average_recall:.4f}')\n",
        "print(f'F1 Score: {average_f1:.4f}')\n"
      ],
      "metadata": {
        "id": "gpSj2AFwdJeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, _tree\n",
        "\n",
        "def describe_tree_splits(tree, feature_names):\n",
        "    tree_ = tree.tree_\n",
        "    num_original_features = len(feature_names) // 2  # Assuming feature names are doubled for source and target\n",
        "    feature_name = [\n",
        "        (f\"{feature_names[i]} (source)\" if i < num_original_features else f\"{feature_names[i]} (target)\")\n",
        "        if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
        "        for i in tree_.feature\n",
        "    ]\n",
        "\n",
        "    splits_description = []\n",
        "\n",
        "    def recurse(node, depth):\n",
        "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
        "            name = feature_name[node]\n",
        "            value = tree_.value[node]\n",
        "\n",
        "            # Class distribution at the current node\n",
        "            class_counts = value[0]\n",
        "            total_samples = class_counts.sum()\n",
        "            class_percentages = class_counts / total_samples * 100\n",
        "\n",
        "            splits_description.append(\n",
        "                f\"Split: {name} (samples: {total_samples:.0f}, \"\n",
        "                f\"class percentages: [Class 0: {class_percentages[0]:.2f}%, \"\n",
        "                f\"Class 1: {class_percentages[1]:.2f}%])\"\n",
        "            )\n",
        "\n",
        "            # Traverse the left and right child nodes\n",
        "            recurse(tree_.children_left[node], depth + 1)\n",
        "            recurse(tree_.children_right[node], depth + 1)\n",
        "        else:\n",
        "            value = tree_.value[node]\n",
        "            class_counts = value[0]\n",
        "            total_samples = class_counts.sum()\n",
        "            class_percentages = class_counts / total_samples * 100\n",
        "            splits_description.append(\n",
        "                f\"Leaf: (samples: {total_samples:.0f}, \"\n",
        "                f\"class percentages: [Class 0: {class_percentages[0]:.2f}%, \"\n",
        "                f\"Class 1: {class_percentages[1]:.2f}%])\"\n",
        "            )\n",
        "\n",
        "    recurse(0, 0)\n",
        "    return splits_description\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_decision_tree_with_splits(train_features, train_labels, feature_names=None):\n",
        "    clf = DecisionTreeClassifier(random_state=42)\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    # Ensure feature_names list matches the number of features in the classifier\n",
        "    num_features = train_features.shape[1]\n",
        "    if feature_names is not None and len(feature_names) != num_features:\n",
        "        print(f\"Warning: Feature names list length ({len(feature_names)}) does not match number of features ({num_features})\")\n",
        "        feature_names = [f'Feature_{i}' for i in range(num_features)]\n",
        "\n",
        "    # Describe the splits in the decision tree\n",
        "    try:\n",
        "        splits_description = describe_tree_splits(clf, feature_names)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error describing tree splits: {e}\")\n",
        "        splits_description = None\n",
        "\n",
        "    return splits_description\n",
        "\n",
        "# Test the function\n",
        "num_splits = 1\n",
        "all_splits_descriptions = []\n",
        "\n",
        "for _ in range(num_splits):\n",
        "    data_random_split = Data(x=x, edge_index=edge_index)\n",
        "    split = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
        "    train_data, val_data, test_data = split(data_random_split)\n",
        "\n",
        "    # Prepare features and labels for training\n",
        "    train_features, train_labels, _ = prepare_data_with_node_mapping(train_data.x.numpy(), train_data.edge_index.numpy())\n",
        "\n",
        "    # Generate feature names for both source and destination\n",
        "    original_feature_names = data_features_2.columns.tolist()\n",
        "    feature_names = original_feature_names * 2  # Duplicate for source and destination features\n",
        "\n",
        "    # Evaluate the decision tree\n",
        "    splits_description = test_decision_tree_with_splits(train_features, train_labels, feature_names)\n",
        "    if splits_description is not None:\n",
        "        all_splits_descriptions.append(splits_description)\n",
        "\n",
        "# Print the splits description for each decision tree\n",
        "for i, splits in enumerate(all_splits_descriptions):\n",
        "    print(f'\\nDecision Tree Splits Description for Split {i+1}:')\n",
        "    for split in splits:\n",
        "        print(split)\n"
      ],
      "metadata": {
        "id": "tjoXbbhgrhdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSage Model with binary cross entropy loss"
      ],
      "metadata": {
        "id": "WTO1fyAOFcbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.optim import AdamW\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define GraphSAGE model\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        self.conv_out = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv_out(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, z, pos_edge_index, neg_edge_index):\n",
        "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "        return torch.sigmoid(self.decode(z, edge_index))\n",
        "\n",
        "def train_model(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "\n",
        "    pos_edge_index = data.edge_index\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=data.edge_index,\n",
        "        num_nodes=data.num_nodes,\n",
        "        num_neg_samples=data.edge_index.size(1)\n",
        "    )\n",
        "\n",
        "    pos_pred = torch.sigmoid(model.decode(z, pos_edge_index))\n",
        "    neg_pred = torch.sigmoid(model.decode(z, neg_edge_index))\n",
        "\n",
        "    pos_labels = torch.ones(pos_pred.size(0), dtype=torch.float).to(device)\n",
        "    neg_labels = torch.zeros(neg_pred.size(0), dtype=torch.float).to(device)\n",
        "\n",
        "    pos_loss = -torch.log(pos_pred).mean()\n",
        "    neg_loss = -torch.log(1 - neg_pred).mean()\n",
        "    loss = pos_loss + neg_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    all_preds = torch.cat([pos_pred, neg_pred])\n",
        "    all_labels = torch.cat([pos_labels, neg_labels])\n",
        "\n",
        "    all_preds_binary = (all_preds > 0.5).float()\n",
        "    accuracy = (all_preds_binary == all_labels).float().mean().item()\n",
        "\n",
        "    return loss.item(), accuracy, z\n",
        "\n",
        "def clear_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        shutil.rmtree(directory)\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_channels = 87\n",
        "lr = 0.0006319720394446876\n",
        "num_layers = 3\n",
        "dropout = 0.09011363149848563\n",
        "epochs = 6465\n",
        "weight_decay = 1.5091468730235952e-09\n",
        "out_channels = 7\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "x = torch.tensor(df_reindexed.values.astype(np.float32))\n",
        "edge_index = torch.tensor([df['pid'].values, df['alter'].values], dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "split = RandomLinkSplit(num_val=0.1, num_test=0.1)\n",
        "train_data, val_data, test_data = split(data)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data_selected = Data(x=x, edge_index=train_data.edge_index)\n",
        "val_data_selected = Data(x=x, edge_index=val_data.edge_index)\n",
        "test_data_selected = Data(x=x, edge_index=test_data.edge_index)\n",
        "\n",
        "model = GraphSAGE(\n",
        "    in_channels=x.size(1),\n",
        "    hidden_channels=hidden_channels,\n",
        "    out_channels=out_channels,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss, accuracy, embeddings = train_model(model, train_data_selected, optimizer)\n",
        "    if epoch % 100 == 0:  # Print every 100 epochs\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save embeddings\n",
        "embeddings_save_dir = '/content/embeddings'\n",
        "clear_directory(embeddings_save_dir)\n",
        "\n",
        "embeddings_path = os.path.join(embeddings_save_dir, f\"embeddings.pt\")\n",
        "torch.save(embeddings, embeddings_path)\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n"
      ],
      "metadata": {
        "id": "6VhoxCja_MQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAT implementation\n"
      ],
      "metadata": {
        "id": "1VUd20iq-db_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch.optim import AdamW\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "class GraphGAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, heads):\n",
        "        super(GraphGAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "        self.conv_out = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv_out(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, z, pos_edge_index, neg_edge_index):\n",
        "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "        return torch.sigmoid(self.decode(z, edge_index))\n",
        "\n",
        "def train_model(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "\n",
        "    pos_edge_index = data.edge_index\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=data.edge_index,\n",
        "        num_nodes=data.num_nodes,\n",
        "        num_neg_samples=data.edge_index.size(1)\n",
        "    )\n",
        "\n",
        "    # Calculate the loss\n",
        "    pos_pred = torch.sigmoid(model.decode(z, pos_edge_index))\n",
        "    neg_pred = torch.sigmoid(model.decode(z, neg_edge_index))\n",
        "\n",
        "    pos_loss = -torch.log(pos_pred).mean()\n",
        "    neg_loss = -torch.log(1 - neg_pred).mean()\n",
        "    loss = pos_loss + neg_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    pos_labels = torch.ones(pos_pred.size(0), dtype=torch.float).to(device)\n",
        "    neg_labels = torch.zeros(neg_pred.size(0), dtype=torch.float).to(device)\n",
        "\n",
        "    all_preds = torch.cat([pos_pred, neg_pred])\n",
        "    all_labels = torch.cat([pos_labels, neg_labels])\n",
        "\n",
        "    all_preds_binary = (all_preds > 0.5).float()\n",
        "    accuracy = (all_preds_binary == all_labels).float().mean().item()\n",
        "\n",
        "    return loss.item(), accuracy, z\n",
        "\n",
        "def clear_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        shutil.rmtree(directory)\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_channels = 16\n",
        "lr = 0.000959692457912392\n",
        "num_layers = 1\n",
        "dropout = 0.062244324375886104\n",
        "epochs = 8558\n",
        "weight_decay = 1.7012057072377146e-09\n",
        "out_channels = 7\n",
        "heads = 8\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "x = torch.tensor(df_reindexed.values.astype(np.float32))\n",
        "edge_index = torch.tensor([df['pid'].values, df['alter'].values], dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "split = RandomLinkSplit(num_val=0.2, num_test=0.1)\n",
        "train_data, val_data, test_data = split(data)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data_selected = Data(x=x, edge_index=train_data.edge_index)\n",
        "val_data_selected = Data(x=x, edge_index=val_data.edge_index)\n",
        "test_data_selected = Data(x=x, edge_index=test_data.edge_index)\n",
        "\n",
        "model = GraphGAT(\n",
        "    in_channels=x.size(1),\n",
        "    hidden_channels=hidden_channels,\n",
        "    out_channels=out_channels,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout,\n",
        "    heads = heads\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss, accuracy, embeddings = train_model(model, train_data_selected, optimizer)\n",
        "    if epoch % 100 == 0:  # Print every 100 epochs\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save embeddings\n",
        "embeddings_save_dir = '/content/embeddings'\n",
        "clear_directory(embeddings_save_dir)\n",
        "\n",
        "embeddings_path = os.path.join(embeddings_save_dir, f\"embeddings.pt\")\n",
        "torch.save(embeddings, embeddings_path)\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "\n",
        "embeddings_df = pd.DataFrame(embeddings.detach().cpu().numpy())\n",
        "csv_path = os.path.join(embeddings_save_dir, \"embeddings.csv\")\n",
        "embeddings_df.to_csv(csv_path, index=False)\n",
        "print(f\"Embeddings saved as CSV to {csv_path}\")\n",
        "\n",
        "# Save the GraphGAT model\n",
        "model_save_path = '/content/graphgat_model.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "f6JmUXcg-gUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding visualisation"
      ],
      "metadata": {
        "id": "io03dPX7FY2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "from torch_geometric.utils import to_networkx\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Load the embeddings\n",
        "#embeddings_path = os.path.join(embeddings_save_dir, \"embeddings.pt\")\n",
        "embeddings_path  = '/content/embeddings/embeddings.pt'\n",
        "embeddings = torch.load(embeddings_path)\n",
        "embeddings_np = embeddings.detach().cpu().numpy()\n",
        "\n",
        "# Reduce dimensions to 2D using UMAP\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "embedding_2d = reducer.fit_transform(embeddings_np)\n",
        "\n",
        "# Plot the 2D embeddings\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], s=60, cmap='Spectral', alpha=1)\n",
        "\n",
        "# Define edges and lines for connections\n",
        "edges = edge_index.t().cpu().numpy()\n",
        "lines = []\n",
        "line_widths = []\n",
        "connection_counts = np.zeros(len(embedding_2d))  # Track the number of connections for each point\n",
        "\n",
        "# Calculate the distances between connected embeddings\n",
        "for edge in edges:\n",
        "    point1 = embedding_2d[edge[0]]\n",
        "    point2 = embedding_2d[edge[1]]\n",
        "    distance = np.linalg.norm(point1 - point2)\n",
        "\n",
        "    # Invert the distance to get a thicker line for closer embeddings\n",
        "    linewidth = 0.5 / (distance + 1e-5)  # Reduce the line thickness\n",
        "    lines.append((point1, point2))\n",
        "    line_widths.append(linewidth)\n",
        "\n",
        "    # Count the number of connections for each point\n",
        "    connection_counts[edge[0]] += 1\n",
        "    connection_counts[edge[1]] += 1\n",
        "\n",
        "# Add the lines to the main plot\n",
        "lc = LineCollection(lines, color='gray', alpha=1, linewidths=line_widths)\n",
        "plt.gca().add_collection(lc)\n",
        "\n",
        "plt.title('UMAP Projection of the Embedding Space with Connections', fontsize=20)\n",
        "plt.xlabel('UMAP 1', fontsize=24)\n",
        "plt.ylabel('UMAP 2', fontsize=24)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "\n",
        "# Create three inset plots: many, few, and normal connections\n",
        "ax_inset_many = plt.gca().inset_axes([0.1, 0.5, 0.3, 0.3])  # Inset plot for many connections (left side)\n",
        "ax_inset_few = plt.gca().inset_axes([0.6, 0.2, 0.2, 0.2])  # Inset plot for few connections (bottom right)\n",
        "ax_inset_normal = plt.gca().inset_axes([0.1, 0.2, 0.2, 0.2])  # Inset plot for normal connections (top right)\n",
        "\n",
        "# Find the index for a cluster with many connections\n",
        "cluster_many_idx = np.argmax(connection_counts)  # Cluster with the most connections\n",
        "\n",
        "# Find the index for a cluster with few connections (at least 2-3 connections)\n",
        "few_connections_idxs = np.where(connection_counts >= 2)[0]  # Clusters with at least 2 connections\n",
        "cluster_few_idx = few_connections_idxs[np.argmin(connection_counts[few_connections_idxs])]  # Cluster with the fewest among those with at least 2 connections\n",
        "\n",
        "# Find the index for a cluster with a normal amount of connections\n",
        "median_connections = np.median(connection_counts)\n",
        "normal_connections_idxs = np.where(connection_counts > 2)[0]  # Clusters with more than 2 connections\n",
        "normal_connections_diffs = np.abs(connection_counts[normal_connections_idxs] - median_connections)  # Difference from median\n",
        "cluster_normal_idx = normal_connections_idxs[np.argmin(normal_connections_diffs)]  # Cluster closest to median\n",
        "\n",
        "# Define zoom level and limits for the inset plots\n",
        "zoom_level = 0.3\n",
        "\n",
        "# Inset plot for many connections\n",
        "center_point_many = embedding_2d[cluster_many_idx]\n",
        "xlim_many = [center_point_many[0] - zoom_level, center_point_many[0] + zoom_level]\n",
        "ylim_many = [center_point_many[1] - zoom_level, center_point_many[1] + zoom_level]\n",
        "\n",
        "ax_inset_many.scatter(embedding_2d[:, 0], embedding_2d[:, 1], s=100, cmap='Spectral', alpha=1)\n",
        "ax_inset_many.set_xlim(xlim_many)\n",
        "ax_inset_many.set_ylim(ylim_many)\n",
        "\n",
        "# Add lines for connections in the 'many connections' inset plot\n",
        "cluster_lines_many = []\n",
        "cluster_line_widths_many = []\n",
        "for line, lw in zip(lines, line_widths):\n",
        "    if (xlim_many[0] <= line[0][0] <= xlim_many[1] and ylim_many[0] <= line[0][1] <= ylim_many[1]) or \\\n",
        "       (xlim_many[0] <= line[1][0] <= xlim_many[1] and ylim_many[0] <= line[1][1] <= ylim_many[1]):\n",
        "        cluster_lines_many.append(line)\n",
        "        cluster_line_widths_many.append(lw * 0.2)  # Further reduce line thickness in the inset\n",
        "\n",
        "lc_many = LineCollection(cluster_lines_many, color='black', alpha=1, linewidths=cluster_line_widths_many)\n",
        "ax_inset_many.add_collection(lc_many)\n",
        "\n",
        "ax_inset_many.set_xticks([])\n",
        "ax_inset_many.set_yticks([])\n",
        "\n",
        "# Inset plot for few connections\n",
        "center_point_few = embedding_2d[cluster_few_idx]\n",
        "xlim_few = [center_point_few[0] - zoom_level, center_point_few[0] + zoom_level]\n",
        "ylim_few = [center_point_few[1] - zoom_level, center_point_few[1] + zoom_level]\n",
        "\n",
        "ax_inset_few.scatter(embedding_2d[:, 0], embedding_2d[:, 1], s=100, cmap='Spectral', alpha=1)\n",
        "ax_inset_few.set_xlim(xlim_few)\n",
        "ax_inset_few.set_ylim(ylim_few)\n",
        "\n",
        "# Add lines for connections in the 'few connections' inset plot\n",
        "cluster_lines_few = []\n",
        "cluster_line_widths_few = []\n",
        "for line, lw in zip(lines, line_widths):\n",
        "    if (xlim_few[0] <= line[0][0] <= xlim_few[1] and ylim_few[0] <= line[0][1] <= ylim_few[1]) or \\\n",
        "       (xlim_few[0] <= line[1][0] <= xlim_few[1] and ylim_few[0] <= line[1][1] <= ylim_few[1]):\n",
        "        cluster_lines_few.append(line)\n",
        "        cluster_line_widths_few.append(max(lw * 0.5, 0.5))  # Ensure line thickness is visible\n",
        "\n",
        "lc_few = LineCollection(cluster_lines_few, color='black', alpha=1, linewidths=cluster_line_widths_few)\n",
        "ax_inset_few.add_collection(lc_few)\n",
        "\n",
        "ax_inset_few.set_xticks([])\n",
        "ax_inset_few.set_yticks([])\n",
        "\n",
        "\n",
        "\n",
        "# Inset plot for normal connections\n",
        "center_point_normal = embedding_2d[cluster_normal_idx]\n",
        "xlim_normal = [center_point_normal[0] - zoom_level, center_point_normal[0] + zoom_level]\n",
        "ylim_normal = [center_point_normal[1] - zoom_level, center_point_normal[1] + zoom_level]\n",
        "\n",
        "ax_inset_normal.scatter(embedding_2d[:, 0], embedding_2d[:, 1], s=100, cmap='Spectral', alpha=1)\n",
        "ax_inset_normal.set_xlim(xlim_normal)\n",
        "ax_inset_normal.set_ylim(ylim_normal)\n",
        "\n",
        "# Add lines for connections in the 'normal connections' inset plot\n",
        "cluster_lines_normal = []\n",
        "cluster_line_widths_normal = []\n",
        "for line, lw in zip(lines, line_widths):\n",
        "    if (xlim_normal[0] <= line[0][0] <= xlim_normal[1] and ylim_normal[0] <= line[0][1] <= ylim_normal[1]) or \\\n",
        "       (xlim_normal[0] <= line[1][0] <= xlim_normal[1] and ylim_normal[0] <= line[1][1] <= ylim_normal[1]):\n",
        "        cluster_lines_normal.append(line)\n",
        "        cluster_line_widths_normal.append(lw * 0.2)  # Further reduce line thickness in the inset\n",
        "\n",
        "lc_normal = LineCollection(cluster_lines_normal, color='black', alpha=1, linewidths=cluster_line_widths_normal)\n",
        "\n",
        "lc_normal = LineCollection(cluster_lines_normal, color='black', alpha=1, linewidths=cluster_line_widths_normal)\n",
        "ax_inset_normal.add_collection(lc_normal)\n",
        "\n",
        "ax_inset_normal.set_xticks([])\n",
        "ax_inset_normal.set_yticks([])\n",
        "\n",
        "plt.annotate('', xy=(center_point_many[0], center_point_many[1]), xytext=(0.1, 0.8),\n",
        "             xycoords='data', textcoords='axes fraction',\n",
        "             arrowprops=dict(facecolor='red', arrowstyle='->'), fontsize=12)\n",
        "\n",
        "# Draw an arrow to point to the cluster with few connections\n",
        "plt.annotate('', xy=(center_point_few[0], center_point_few[1]), xytext=(0.8, 0.3),\n",
        "             xycoords='data', textcoords='axes fraction',\n",
        "             arrowprops=dict(facecolor='red', arrowstyle='->'), fontsize=12)\n",
        "\n",
        "# Draw an arrow to point to the cluster with normal connections\n",
        "plt.annotate('', xy=(center_point_normal[0], center_point_normal[1]), xytext=(0.2, 0.2),\n",
        "             xycoords='data', textcoords='axes fraction',\n",
        "             arrowprops=dict(facecolor='red', arrowstyle='->'), fontsize=12)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HYyKaiAvBf6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Node disambiguation (Evaluation pipeline 1)"
      ],
      "metadata": {
        "id": "OxA-oc5EFSzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import umap\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "#embeddings_path = os.path.join(embeddings_save_dir, \"embeddings.pt\")\n",
        "embeddings_path = '/content/embeddings/embeddings.pt'\n",
        "embeddings = torch.load(embeddings_path)\n",
        "embeddings_np = embeddings.detach().cpu().numpy()\n",
        "\n",
        "reducer = umap.UMAP(n_components=3, random_state=42)\n",
        "embedding_3d = reducer.fit_transform(embeddings_np)\n",
        "\n",
        "edges = edge_index.t().cpu().numpy()\n",
        "\n",
        "linked_distances = []\n",
        "for edge in edges:\n",
        "    dist = np.linalg.norm(embedding_3d[edge[0]] - embedding_3d[edge[1]])\n",
        "    linked_distances.append(dist)\n",
        "\n",
        "linked_distances = np.array(linked_distances)\n",
        "\n",
        "num_nodes = embedding_3d.shape[0]\n",
        "all_pairs = np.array(np.triu_indices(num_nodes, k=1)).T\n",
        "\n",
        "other_pairs = [pair for pair in all_pairs if not any((pair == edge).all() for edge in edges)]\n",
        "other_pairs = np.array(other_pairs)\n",
        "\n",
        "other_distances = []\n",
        "for pair in other_pairs:\n",
        "    dist = np.linalg.norm(embedding_3d[pair[0]] - embedding_3d[pair[1]])\n",
        "    other_distances.append(dist)\n",
        "\n",
        "other_distances = np.array(other_distances)\n",
        "\n",
        "linked_mean = np.mean(linked_distances)\n",
        "linked_std = np.std(linked_distances)\n",
        "other_mean = np.mean(other_distances)\n",
        "other_std = np.std(other_distances)\n",
        "\n",
        "t_stat, p_value = ttest_ind(linked_distances, other_distances)\n",
        "\n",
        "print(f\"Linked pairs - Mean distance: {linked_mean}, Std Dev: {linked_std}\")\n",
        "print(f\"Other pairs - Mean distance: {other_mean}, Std Dev: {other_std}\")\n",
        "print(f\"T-test - t-statistic: {t_stat}, p-value: {p_value}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.hist(linked_distances, bins=50, alpha=0.5, label='Linked Pairs')\n",
        "plt.hist(other_distances, bins=50, alpha=0.5, label='Other Pairs')\n",
        "plt.legend()\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distance Distribution in Embedding Space')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e-xZ8jiTEbTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "csv_path = os.path.join(embeddings_save_dir, \"embeddings.csv\")\n",
        "embeddings_df.to_csv(csv_path, index=False)\n",
        "print(f\"Embeddings saved as CSV to {csv_path}\")\n"
      ],
      "metadata": {
        "id": "vldUaNWOtWpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pair disambiguation Simulation"
      ],
      "metadata": {
        "id": "uNaMyDyyXbjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embeddings.detach().numpy()"
      ],
      "metadata": {
        "id": "Wdo88djMv2RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For GraphSage and GAT\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def onevsallpipeline1(embeddings, edgelist):\n",
        "    edges = [(edgelist[0, i].item(), edgelist[1, i].item()) for i in range(edgelist.size(1))]\n",
        "    nodes = np.unique(np.concatenate([edgelist[0].numpy(), edgelist[1].numpy()]))\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_scores = []\n",
        "\n",
        "    for edge in edges:\n",
        "        u = edge[0]\n",
        "        v1 = edge[1]\n",
        "\n",
        "        not_connected_nodes = [v for v in nodes if (u, v) not in edges]\n",
        "\n",
        "        for v2 in not_connected_nodes:\n",
        "            emb_u = embeddings[u]\n",
        "            emb_v1 = embeddings[v1]\n",
        "            emb_v2 = embeddings[v2]\n",
        "\n",
        "            # Compute euclidean distance between (u, v1) and (u, v2)\n",
        "            dist1 = np.linalg.norm(emb_u - emb_v1)\n",
        "            dist2 = np.linalg.norm(emb_u - emb_v2)\n",
        "\n",
        "            # Assign score based on whether v1 or v2 is more likely\n",
        "            true_label = 1  # Since v1 is the actual connected node\n",
        "            predicted_score = 1 if dist1 < dist2 else 0\n",
        "\n",
        "            true_labels.append(true_label)\n",
        "            predicted_scores.append(predicted_score)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_scores)\n",
        "    precision = precision_score(true_labels, predicted_scores)\n",
        "    recall = recall_score(true_labels, predicted_scores)\n",
        "    f1 = f1_score(true_labels, predicted_scores)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(\"{:.4f}\".format(accuracy)),\n",
        "        \"precision\": float(\"{:.4f}\".format(precision)),\n",
        "        \"recall\": float(\"{:.4f}\".format(recall)),\n",
        "        \"f1_score\": float(\"{:.4f}\".format(f1))\n",
        "    }\n"
      ],
      "metadata": {
        "id": "k_zw_akmC82H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onevsallpipeline1(embeddings, edge_index)"
      ],
      "metadata": {
        "id": "DiNcS_bdC9Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For Decision tree\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import pandas as pd\n",
        "\n",
        "def prepare_data_with_node_mapping(x, edge_index):\n",
        "    pos_edge_index = edge_index.T\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=torch.tensor(edge_index),\n",
        "        num_nodes=x.shape[0],\n",
        "        num_neg_samples=pos_edge_index.shape[0]\n",
        "    ).T\n",
        "\n",
        "    all_edges = np.concatenate([pos_edge_index, neg_edge_index], axis=0)\n",
        "    labels = np.concatenate([np.ones(pos_edge_index.shape[0]), np.zeros(neg_edge_index.shape[0])])\n",
        "\n",
        "    features = np.hstack([x[all_edges[:, 0]], x[all_edges[:, 1]]])\n",
        "\n",
        "    # Create a mapping of feature indices to nodes\n",
        "    node_mapping = []\n",
        "    for edge in all_edges:\n",
        "        node_mapping.append((edge[0], edge[1]))\n",
        "\n",
        "    return features, labels, node_mapping\n",
        "\n",
        "def test_decision_tree(train_features, train_labels, test_features, test_labels, feature_names=None, node_mapping=None):\n",
        "    clf = DecisionTreeClassifier(random_state=42)\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    test_preds = clf.predict(test_features)\n",
        "    test_probs = clf.predict_proba(test_features)\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, test_preds)\n",
        "    precision = precision_score(test_labels, test_preds)\n",
        "    recall = recall_score(test_labels, test_preds)\n",
        "    f1 = f1_score(test_labels, test_preds)\n",
        "    auc = roc_auc_score(test_labels, test_probs[:, 1])\n",
        "\n",
        "    importances = clf.feature_importances_\n",
        "    importance_indices = np.argsort(importances)[::-1]\n",
        "    relevant_indices = [i for i in importance_indices if importances[i] > 0]\n",
        "    top_feature_indices = relevant_indices[:10]\n",
        "    top_feature_importances = importances[top_feature_indices]\n",
        "\n",
        "    if feature_names is not None:\n",
        "        feature_names = feature_names[:len(train_features[0])]\n",
        "        top_feature_names = [feature_names[i] for i in top_feature_indices if i < len(feature_names)]\n",
        "    else:\n",
        "        top_feature_names = [f'Feature_{i}' for i in top_feature_indices]\n",
        "\n",
        "    if node_mapping is not None:\n",
        "        top_feature_node_mapping = [(node_mapping[i][0], node_mapping[i][1]) for i in top_feature_indices if i < len(node_mapping)]\n",
        "    else:\n",
        "        top_feature_node_mapping = None\n",
        "\n",
        "    try:\n",
        "        tree_rules = export_text(clf, feature_names=feature_names)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error exporting tree rules: {e}\")\n",
        "        tree_rules = None\n",
        "\n",
        "    return auc, accuracy, precision, recall, f1, top_feature_names, top_feature_importances, top_feature_node_mapping, tree_rules, clf\n",
        "\n",
        "def onevsallpipeline1(clf, edge_index, x):\n",
        "    edges = [(edge_index[0, i].item(), edge_index[1, i].item()) for i in range(edge_index.size(1))]\n",
        "    nodes = np.unique(np.concatenate([edge_index[0].numpy(), edge_index[1].numpy()]))\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_scores = []\n",
        "\n",
        "    for edge in edges:\n",
        "        u = edge[0]\n",
        "        v1 = edge[1]\n",
        "\n",
        "        not_connected_nodes = [v for v in nodes if (u, v) not in edges]\n",
        "\n",
        "        for v2 in not_connected_nodes:\n",
        "\n",
        "            feature_u_v1 = np.hstack([x[u], x[v1]])\n",
        "            feature_u_v2 = np.hstack([x[u], x[v2]])\n",
        "\n",
        "            prob_v1 = clf.predict_proba([feature_u_v1])[0][1]\n",
        "            prob_v2 = clf.predict_proba([feature_u_v2])[0][1]\n",
        "\n",
        "            true_label = 1\n",
        "            predicted_score = 1 if prob_v1 > prob_v2 else 0\n",
        "\n",
        "            true_labels.append(true_label)\n",
        "            predicted_scores.append(predicted_score)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_scores)\n",
        "    precision = precision_score(true_labels, predicted_scores)\n",
        "    recall = recall_score(true_labels, predicted_scores)\n",
        "    f1 = f1_score(true_labels, predicted_scores)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(\"{:.4f}\".format(accuracy)),\n",
        "        \"precision\": float(\"{:.4f}\".format(precision)),\n",
        "        \"recall\": float(\"{:.4f}\".format(recall)),\n",
        "        \"f1_score\": float(\"{:.4f}\".format(f1))\n",
        "    }\n",
        "\n",
        "# Test the function\n",
        "num_splits = 10\n",
        "total_auc, total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0, 0\n",
        "all_top_feature_names = []\n",
        "all_top_feature_importances = []\n",
        "all_top_feature_node_mapping = []\n",
        "all_tree_rules = []\n",
        "\n",
        "for _ in range(num_splits):\n",
        "    data_random_split = Data(x=x, edge_index=edge_index)\n",
        "    split = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
        "    train_data, val_data, test_data = split(data_random_split)\n",
        "\n",
        "    train_features, train_labels, train_node_mapping = prepare_data_with_node_mapping(train_data.x.numpy(), train_data.edge_index.numpy())\n",
        "    test_features, test_labels, test_node_mapping = prepare_data_with_node_mapping(test_data.x.numpy(), test_data.edge_index.numpy())\n",
        "\n",
        "    feature_names = data_features_2.columns.tolist()\n",
        "\n",
        "    auc, accuracy, precision, recall, f1, top_feature_names, top_feature_importances, top_feature_node_mapping, tree_rules, clf = test_decision_tree(\n",
        "        train_features, train_labels, test_features, test_labels, feature_names, test_node_mapping\n",
        "    )\n",
        "    total_auc += auc\n",
        "    total_accuracy += accuracy\n",
        "    total_precision += precision\n",
        "    total_recall += recall\n",
        "    total_f1 += f1\n",
        "\n",
        "    if top_feature_names is not None:\n",
        "        all_top_feature_names.extend(top_feature_names)\n",
        "        all_top_feature_importances.extend(top_feature_importances)\n",
        "        all_top_feature_node_mapping.extend(top_feature_node_mapping)\n",
        "\n",
        "    if tree_rules is not None:\n",
        "        all_tree_rules.append(tree_rules)\n",
        "\n",
        "    simulation_results = onevsallpipeline1(clf, test_data.edge_index, test_data.x.numpy())\n",
        "\n",
        "    print(f'Simulation Results - Accuracy: {simulation_results[\"accuracy\"]}, Precision: {simulation_results[\"precision\"]}, Recall: {simulation_results[\"recall\"]}, F1 Score: {simulation_results[\"f1_score\"]}')\n",
        "\n",
        "average_auc = total_auc / num_splits\n",
        "average_accuracy = total_accuracy / num_splits\n",
        "average_precision = total_precision / num_splits\n",
        "average_recall = total_recall / num_splits\n",
        "average_f1 = total_f1 / num_splits\n",
        "\n",
        "print(f'Average Metrics on Random Splits of Test Graphs - AUC: {average_auc:.4f}, Accuracy: {average_accuracy:.4f}, Precision: {average_precision:.4f}, Recall: {average_recall:.4f}, F1 Score: {average_f1:.4f}')\n",
        "\n",
        "top_feature_names, top_feature_importances = zip(*sorted(zip(all_top_feature_names, all_top_feature_importances), key=lambda x: x[1], reverse=True))\n",
        "top_feature_names = list(top_feature_names)\n",
        "top_feature_importances = list(top_feature_importances)\n",
        "\n",
        "top_features = list(zip(top_feature_names, top_feature_importances))\n",
        "top_features = [f for f in top_features if f[1] > 0][:10]\n",
        "top_feature_names, top_feature_importances = zip(*top_features) if top_features else ([], [])\n",
        "\n",
        "print(f'Top Features - Names: {top_feature_names}, Importances: {top_feature_importances}')\n",
        "\n",
        "for i, rules in enumerate(all_tree_rules):\n",
        "    print(f'\\nDecision Tree Rules for Split {i+1}:')\n",
        "    print(rules)\n"
      ],
      "metadata": {
        "id": "Xb7nMF8Iph4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ Prepare Data Function\n",
        "def prepare_data_with_node_mapping(x, edge_index):\n",
        "    pos_edge_index = edge_index.T\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=torch.tensor(edge_index),\n",
        "        num_nodes=x.shape[0],\n",
        "        num_neg_samples=pos_edge_index.shape[0]\n",
        "    ).T\n",
        "\n",
        "    all_edges = np.concatenate([pos_edge_index, neg_edge_index], axis=0)\n",
        "    labels = np.concatenate([np.ones(pos_edge_index.shape[0]), np.zeros(neg_edge_index.shape[0])])\n",
        "\n",
        "    features = np.hstack([x[all_edges[:, 0]], x[all_edges[:, 1]]])\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# ✅ MLP Training & Evaluation Function\n",
        "def test_mlp(train_features, train_labels, test_features, test_labels):\n",
        "    clf = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', random_state=42, max_iter=200)\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    test_preds = clf.predict(test_features)\n",
        "    test_probs = clf.predict_proba(test_features)[:, 1]  # Get probabilities for AUC calculation\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, test_preds)\n",
        "    precision = precision_score(test_labels, test_preds)\n",
        "    recall = recall_score(test_labels, test_preds)\n",
        "    f1 = f1_score(test_labels, test_preds)\n",
        "    auc = roc_auc_score(test_labels, test_probs)\n",
        "\n",
        "    return auc, accuracy, precision, recall, f1, clf\n",
        "\n",
        "# ✅ One-vs-All Pipeline Function\n",
        "def onevsallpipeline1(clf, edge_index, x):\n",
        "    edges = [(edge_index[0, i].item(), edge_index[1, i].item()) for i in range(edge_index.size(1))]\n",
        "    nodes = np.unique(np.concatenate([edge_index[0].numpy(), edge_index[1].numpy()]))\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_scores = []\n",
        "\n",
        "    for edge in edges:\n",
        "        u = edge[0]\n",
        "        v1 = edge[1]\n",
        "\n",
        "        not_connected_nodes = [v for v in nodes if (u, v) not in edges]\n",
        "\n",
        "        for v2 in not_connected_nodes:\n",
        "            feature_u_v1 = np.hstack([x[u], x[v1]])\n",
        "            feature_u_v2 = np.hstack([x[u], x[v2]])\n",
        "\n",
        "            prob_v1 = clf.predict_proba([feature_u_v1])[0][1]\n",
        "            prob_v2 = clf.predict_proba([feature_u_v2])[0][1]\n",
        "\n",
        "            true_label = 1\n",
        "            predicted_score = 1 if prob_v1 > prob_v2 else 0\n",
        "\n",
        "            true_labels.append(true_label)\n",
        "            predicted_scores.append(predicted_score)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_scores)\n",
        "    precision = precision_score(true_labels, predicted_scores)\n",
        "    recall = recall_score(true_labels, predicted_scores)\n",
        "    f1 = f1_score(true_labels, predicted_scores)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(\"{:.4f}\".format(accuracy)),\n",
        "        \"precision\": float(\"{:.4f}\".format(precision)),\n",
        "        \"recall\": float(\"{:.4f}\".format(recall)),\n",
        "        \"f1_score\": float(\"{:.4f}\".format(f1))\n",
        "    }\n",
        "\n",
        "# ✅ Run Multiple Splits & Compute Average Metrics\n",
        "num_splits = 10\n",
        "total_auc, total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0, 0\n",
        "\n",
        "for _ in range(num_splits):\n",
        "    data_random_split = Data(x=x, edge_index=edge_index)\n",
        "    split = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
        "    train_data, val_data, test_data = split(data_random_split)\n",
        "\n",
        "    # Prepare features and labels for training and testing\n",
        "    train_features, train_labels = prepare_data_with_node_mapping(train_data.x.numpy(), train_data.edge_index.numpy())\n",
        "    test_features, test_labels = prepare_data_with_node_mapping(test_data.x.numpy(), test_data.edge_index.numpy())\n",
        "\n",
        "    # Train & Evaluate MLP\n",
        "    auc, accuracy, precision, recall, f1, clf = test_mlp(train_features, train_labels, test_features, test_labels)\n",
        "\n",
        "    # Aggregate Metrics\n",
        "    total_auc += auc\n",
        "    total_accuracy += accuracy\n",
        "    total_precision += precision\n",
        "    total_recall += recall\n",
        "    total_f1 += f1\n",
        "\n",
        "    # ✅ Run One-vs-All Evaluation\n",
        "    simulation_results = onevsallpipeline1(clf, test_data.edge_index, test_data.x.numpy())\n",
        "\n",
        "    print(f'Simulation Results - Accuracy: {simulation_results[\"accuracy\"]}, Precision: {simulation_results[\"precision\"]}, Recall: {simulation_results[\"recall\"]}, F1 Score: {simulation_results[\"f1_score\"]}')\n",
        "\n",
        "# ✅ Compute Averages Across Splits\n",
        "average_auc = total_auc / num_splits\n",
        "average_accuracy = total_accuracy / num_splits\n",
        "average_precision = total_precision / num_splits\n",
        "average_recall = total_recall / num_splits\n",
        "average_f1 = total_f1 / num_splits\n",
        "\n",
        "# ✅ Print Results\n",
        "print(f'Average Metrics on Random Splits of Test Graphs:')\n",
        "print(f'AUC: {average_auc:.4f}')\n",
        "print(f'Accuracy: {average_accuracy:.4f}')\n",
        "print(f'Precision: {average_precision:.4f}')\n",
        "print(f'Recall: {average_recall:.4f}')\n",
        "print(f'F1 Score: {average_f1:.4f}')\n"
      ],
      "metadata": {
        "id": "3WOtEKHkeWAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link Existence Simulation"
      ],
      "metadata": {
        "id": "Gj3CEftWMgFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def test(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(data.x.to(device), data.edge_index.to(device))\n",
        "\n",
        "        pos_edge_index = data.edge_label_index[:, data.edge_label == 1].to(device)\n",
        "        neg_edge_index = data.edge_label_index[:, data.edge_label == 0].to(device)\n",
        "\n",
        "        pos_y = torch.ones(pos_edge_index.size(1)).to(device)\n",
        "        neg_y = torch.zeros(neg_edge_index.size(1)).to(device)\n",
        "        y = torch.cat([pos_y, neg_y], dim=0).to(device)\n",
        "\n",
        "        pos_pred = model.decode(z, pos_edge_index)\n",
        "        neg_pred = model.decode(z, neg_edge_index)\n",
        "        pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
        "\n",
        "        if torch.isnan(pred).any():\n",
        "            return float('-inf'), float('-inf'), float('-inf'), float('-inf'), float('-inf')\n",
        "\n",
        "        auc = roc_auc_score(y.cpu().numpy(), pred.cpu().detach().numpy())\n",
        "        pred_labels = (pred > 0.5).float()  # Change to 0.5 threshold\n",
        "        accuracy = accuracy_score(y.cpu().numpy(), pred_labels.cpu().detach().numpy())\n",
        "        precision = precision_score(y.cpu().numpy(), pred_labels.cpu().detach().numpy())\n",
        "        recall = recall_score(y.cpu().numpy(), pred_labels.cpu().detach().numpy())\n",
        "        f1 = f1_score(y.cpu().numpy(), pred_labels.cpu().detach().numpy())\n",
        "\n",
        "    return auc, accuracy, precision, recall, f1\n",
        "\n",
        "def generate_test_graphs(current_data, num_graphs, transform):\n",
        "    test_graphs = []\n",
        "    for _ in range(num_graphs):\n",
        "        train_data, val_data, test_data = transform(current_data)\n",
        "        test_graphs.append(test_data)\n",
        "    return test_graphs\n",
        "\n",
        "num_splits = 10\n",
        "total_auc, total_accuracy, total_precision, total_recall, total_f1 = 0, 0, 0, 0, 0\n",
        "\n",
        "for _ in range(num_splits):\n",
        "    data_random_split = Data(x=x, edge_index=edge_index)\n",
        "    train_data, val_data, test_data = split(data_random_split)\n",
        "\n",
        "    # Ensure test_data contains the necessary attributes\n",
        "    if hasattr(test_data, 'edge_label_index') and hasattr(test_data, 'edge_label'):\n",
        "        test_graphs = generate_test_graphs(data_random_split, num_graphs=num_splits, transform=split)\n",
        "\n",
        "        for test_graph in test_graphs:\n",
        "            auc, accuracy, precision, recall, f1 = test(test_graph, model)\n",
        "            total_auc += auc\n",
        "            total_accuracy += accuracy\n",
        "            total_precision += precision\n",
        "            total_recall += recall\n",
        "            total_f1 += f1\n",
        "\n",
        "average_auc = total_auc / (num_splits * num_splits)\n",
        "average_accuracy = total_accuracy / (num_splits * num_splits)\n",
        "average_precision = total_precision / (num_splits * num_splits)\n",
        "average_recall = total_recall / (num_splits * num_splits)\n",
        "average_f1 = total_f1 / (num_splits * num_splits)\n",
        "\n",
        "print(f'Average Metrics on Random Splits of Test Graphs - AUC: {average_auc:.4f}, Accuracy: {average_accuracy:.4f}, Precision: {average_precision:.4f}, Recall: {average_recall:.4f}, F1 Score: {average_f1:.4f}')\n"
      ],
      "metadata": {
        "id": "9RgpsEAAJkpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "Evaluation Pipeline 1 (Node Disambiguation): 0.9024\n",
        "\n",
        "Pipeline 1 with GAT: 0.9169\n",
        "\n",
        "\n",
        "Pipeline 2 using GAT: Average Metrics on Random Splits of Test Graphs - AUC: 0.8349, Accuracy: 0.8011, Precision: 0.6566, Recall: 0.8439, F1 Score: 0.7362\n",
        "\n",
        "Baseline accuracy with decision tree: 0.69"
      ],
      "metadata": {
        "id": "vikvCY5yLiE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real ambiguous data"
      ],
      "metadata": {
        "id": "vNYbw3IP6lQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ambig_res = pd.read_csv('/ambig_res_edgelist.csv')\n",
        "features = pd.read_csv('/features_final.csv')"
      ],
      "metadata": {
        "id": "sY6IeWH-6qE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.index = features.index.astype(int)\n",
        "new_index = pd.RangeIndex(start=0, stop=251, step=1)\n",
        "df_reindexed = features.reindex(new_index, fill_value=0)\n",
        "bool_columns = features.select_dtypes(include=bool).columns\n",
        "df_reindexed[bool_columns] = df_reindexed[bool_columns].fillna(False)\n",
        "df_reindexed\n",
        "x = torch.tensor(df_reindexed.values.astype(np.float32))\n",
        "x"
      ],
      "metadata": {
        "id": "z3gNj9Ey8Svk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load the CSV file\n",
        "confident_edges = pd.read_csv('/confident_edgelist2.csv')\n",
        "\n",
        "ambig_res = pd.read_csv('/ambig_res_edgelist.csv')\n",
        "pids_to_remove = set(ambig_res['pid'])\n",
        "\n",
        "confident_edges_filtered = confident_edges[~confident_edges['pid'].isin(pids_to_remove)]\n",
        "\n",
        "pids = confident_edges_filtered['pid'].to_numpy()\n",
        "alters = confident_edges_filtered['alter'].to_numpy()\n",
        "\n",
        "edge_index = torch.tensor([pids, alters], dtype=torch.long)\n",
        "\n",
        "print(edge_index)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "noBg0afu78NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GAT instance for real ambiguity in data\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch.optim import AdamW\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "class GraphGAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, heads):\n",
        "        super(GraphGAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "        self.conv_out = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv_out(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, z, pos_edge_index, neg_edge_index):\n",
        "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "        return torch.sigmoid(self.decode(z, edge_index))\n",
        "\n",
        "def train_model(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "\n",
        "    pos_edge_index = data.edge_index\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=data.edge_index,\n",
        "        num_nodes=data.num_nodes,\n",
        "        num_neg_samples=data.edge_index.size(1)\n",
        "    )\n",
        "\n",
        "    # Calculate the loss\n",
        "    pos_pred = torch.sigmoid(model.decode(z, pos_edge_index))\n",
        "    neg_pred = torch.sigmoid(model.decode(z, neg_edge_index))\n",
        "\n",
        "    pos_loss = -torch.log(pos_pred).mean()\n",
        "    neg_loss = -torch.log(1 - neg_pred).mean()\n",
        "    loss = pos_loss + neg_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    pos_labels = torch.ones(pos_pred.size(0), dtype=torch.float).to(device)\n",
        "    neg_labels = torch.zeros(neg_pred.size(0), dtype=torch.float).to(device)\n",
        "\n",
        "    all_preds = torch.cat([pos_pred, neg_pred])\n",
        "    all_labels = torch.cat([pos_labels, neg_labels])\n",
        "\n",
        "    all_preds_binary = (all_preds > 0.5).float()\n",
        "    accuracy = (all_preds_binary == all_labels).float().mean().item()\n",
        "\n",
        "    return loss.item(), accuracy, z\n",
        "\n",
        "def clear_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        shutil.rmtree(directory)\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n",
        "hidden_channels = 16\n",
        "lr = 0.000959692457912392\n",
        "num_layers = 1\n",
        "dropout = 0.062244324375886104\n",
        "epochs = 8558\n",
        "weight_decay = 1.7012057072377146e-09\n",
        "out_channels = 7\n",
        "heads = 8\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "split = RandomLinkSplit(num_val=0.2, num_test=0.1)\n",
        "train_data, val_data, test_data = split(data)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data_selected = Data(x=x, edge_index=train_data.edge_index)\n",
        "val_data_selected = Data(x=x, edge_index=val_data.edge_index)\n",
        "test_data_selected = Data(x=x, edge_index=test_data.edge_index)\n",
        "\n",
        "model = GraphGAT(\n",
        "    in_channels=x.size(1),\n",
        "    hidden_channels=hidden_channels,\n",
        "    out_channels=out_channels,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout,\n",
        "    heads = heads\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss, accuracy, embeddings = train_model(model, train_data_selected, optimizer)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "embeddings_save_dir = '/content/embeddings'\n",
        "clear_directory(embeddings_save_dir)\n",
        "\n",
        "embeddings_path = os.path.join(embeddings_save_dir, f\"embeddings.pt\")\n",
        "torch.save(embeddings, embeddings_path)\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "\n",
        "\n",
        "model_save_path = '/content/graphgat_model.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "6Yt0JNH28oiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embeddings.detach().numpy()\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "KlVbbSan_wvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first 4 rows of ambiguous connections\n",
        "ambig_res_filtered = ambig_res.head(4)\n",
        "ambig_res_filtered.columns"
      ],
      "metadata": {
        "id": "G0qTu2NJA6c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pair disambiguation with real ambiguity in data"
      ],
      "metadata": {
        "id": "12FU0cgIoDNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For GraphSage and GAT\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "def compare_alters(embeddings, ambig_res_subset):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    results = []\n",
        "\n",
        "    for _, row in ambig_res_subset.iterrows():\n",
        "        u = int(row['pid'])\n",
        "        v1 = int(row['real alter'])\n",
        "        v2 = int(row['ambiguous alter'])\n",
        "\n",
        "\n",
        "        if u >= embeddings.shape[0] or v1 >= embeddings.shape[0] or v2 >= embeddings.shape[0]:\n",
        "            print(f\"Skipping PID {u} because one or more nodes are not in embeddings.\")\n",
        "            continue\n",
        "\n",
        "        emb_u = embeddings[u]\n",
        "        emb_v1 = embeddings[v1]\n",
        "        emb_v2 = embeddings[v2]\n",
        "\n",
        "\n",
        "        dist_real_alter = np.linalg.norm(emb_u - emb_v1)\n",
        "        dist_ambiguous_alter = np.linalg.norm(emb_u - emb_v2)\n",
        "\n",
        "\n",
        "        true_label = 1 if dist_real_alter < dist_ambiguous_alter else 0\n",
        "        y_true.append(true_label)\n",
        "\n",
        "\n",
        "        predicted_label = 1 if dist_real_alter < dist_ambiguous_alter else 0\n",
        "        y_pred.append(predicted_label)\n",
        "\n",
        "\n",
        "        results.append({\n",
        "            'pid': u,\n",
        "            'real_alter': v1,\n",
        "            'ambiguous_alter': v2,\n",
        "            'predicted_closer': 'real_alter' if predicted_label == 1 else 'ambiguous_alter',\n",
        "            'true_closer': 'real_alter' if true_label == 1 else 'ambiguous_alter',\n",
        "            'distance_real_alter': dist_real_alter,\n",
        "            'distance_ambiguous_alter': dist_ambiguous_alter\n",
        "        })\n",
        "\n",
        "\n",
        "        print(f\"\\nPID: {u}, Real Alter: {v1}, Ambiguous Alter: {v2}\")\n",
        "        print(f\"Distance to Real Alter: {dist_real_alter:.4f}\")\n",
        "        print(f\"Distance to Ambiguous Alter: {dist_ambiguous_alter:.4f}\")\n",
        "        print(f\"True Closer: {'real_alter' if true_label == 1 else 'ambiguous_alter'}\")\n",
        "        print(f\"Predicted Closer: {'real_alter' if predicted_label == 1 else 'ambiguous_alter'}\")\n",
        "\n",
        "    return np.array(y_true), np.array(y_pred), results\n",
        "\n",
        "\n",
        "if isinstance(embeddings, torch.Tensor):\n",
        "    embeddings = embeddings.detach().numpy()\n",
        "\n",
        "\n",
        "y_true, y_pred, results = compare_alters(embeddings, ambig_res_filtered)\n",
        "\n",
        "\n",
        "precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"\\nMetrics:\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1 Score: {f1:.4f}\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "for result in results:\n",
        "    print(f\"\\nPID: {result['pid']}, Real Alter: {result['real_alter']}, Ambiguous Alter: {result['ambiguous_alter']}\")\n",
        "    print(f\"Predicted Closer: {result['predicted_closer']}, True Closer: {result['true_closer']}\")\n",
        "    print(f\"Distance to Real Alter: {result['distance_real_alter']:.4f}, Distance to Ambiguous Alter: {result['distance_ambiguous_alter']:.4f}\")\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "owvCzuM581bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For Decision Tree\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "features_train = features\n",
        "y_train = np.random.randint(2, size=len(features_train))\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(features_train, y_train)\n",
        "\n",
        "def compare_alters_decision_tree(clf, ambig_res_subset, features):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    results = []\n",
        "\n",
        "    for _, row in ambig_res_subset.iterrows():\n",
        "        u = int(row['pid'])\n",
        "        v1 = int(row['real alter'])\n",
        "        v2 = int(row['ambiguous alter'])\n",
        "\n",
        "        if u not in features.index or v1 not in features.index or v2 not in features.index:\n",
        "            print(f\"Skipping PID {u} or alters {v1} or {v2} as they are not in the feature set.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        features_real_alter = features.loc[v1].values.reshape(1, -1)\n",
        "        features_ambiguous_alter = features.loc[v2].values.reshape(1, -1)\n",
        "\n",
        "\n",
        "        prob_real_alter = clf.predict_proba(features_real_alter)[0, 1]\n",
        "        prob_ambiguous_alter = clf.predict_proba(features_ambiguous_alter)[0, 1]\n",
        "\n",
        "\n",
        "        true_label = 1\n",
        "        y_true.append(true_label)\n",
        "\n",
        "\n",
        "        predicted_label = 1 if prob_real_alter > prob_ambiguous_alter else 0\n",
        "        y_pred.append(predicted_label)\n",
        "\n",
        "\n",
        "        results.append({\n",
        "            'pid': u,\n",
        "            'real_alter': v1,\n",
        "            'ambiguous_alter': v2,\n",
        "            'predicted_closer': 'real_alter' if predicted_label == 1 else 'ambiguous_alter',\n",
        "            'true_closer': 'real_alter',\n",
        "            'probability_real_alter': prob_real_alter,\n",
        "            'probability_ambiguous_alter': prob_ambiguous_alter\n",
        "        })\n",
        "\n",
        "\n",
        "        print(f\"\\nPID: {u}, Real Alter: {v1}, Ambiguous Alter: {v2}\")\n",
        "        print(f\"Probability of Real Alter: {prob_real_alter:.4f}\")\n",
        "        print(f\"Probability of Ambiguous Alter: {prob_ambiguous_alter:.4f}\")\n",
        "        print(f\"Predicted Closer: {'real_alter' if predicted_label == 1 else 'ambiguous_alter'}\")\n",
        "\n",
        "    return np.array(y_true), np.array(y_pred), results\n",
        "\n",
        "\n",
        "y_true, y_pred, results = compare_alters_decision_tree(clf, ambig_res_filtered, features)\n",
        "\n",
        "\n",
        "precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"\\nMetrics:\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1 Score: {f1:.4f}\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "for result in results:\n",
        "    print(f\"\\nPID: {result['pid']}, Real Alter: {result['real_alter']}, Ambiguous Alter: {result['ambiguous_alter']}\")\n",
        "    print(f\"Predicted Closer: {result['predicted_closer']}, True Closer: {result['true_closer']}\")\n",
        "    print(f\"Probability of Real Alter: {result['probability_real_alter']:.4f}, Probability of Ambiguous Alter: {result['probability_ambiguous_alter']:.4f}\")\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "1mfGnJ6GroRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link existence of ambiguous nodes"
      ],
      "metadata": {
        "id": "Klv7ze7rWE2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For GAT and GraphSage\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_save_path = '/content/graphgat_model.pth'\n",
        "embeddings_path = '/content/embeddings/embeddings.pt'\n",
        "\n",
        "\n",
        "model = GraphGAT(\n",
        "    in_channels=x.size(1),\n",
        "    hidden_channels=hidden_channels,\n",
        "    out_channels=out_channels,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout,\n",
        "    heads=heads\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "\n",
        "embeddings = torch.load(embeddings_path)\n",
        "\n",
        "\n",
        "def predict_edge(model, embeddings, edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = torch.sigmoid(model.decode_all(embeddings, edge_index, edge_index)).cpu().numpy()\n",
        "    return preds\n",
        "\n",
        "\n",
        "df_ambiguous = ambig_res\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for _, row in df_ambiguous.iterrows():\n",
        "    pid = row['pid']\n",
        "    real_alter = row['real alter']\n",
        "    ambiguous_alter = row['ambiguous alter']\n",
        "\n",
        "    if real_alter == 0 or ambiguous_alter == 0:\n",
        "\n",
        "        edge_index = torch.tensor([[real_alter, ambiguous_alter], [ambiguous_alter, real_alter]], dtype=torch.long).t().contiguous().to(device)\n",
        "\n",
        "\n",
        "        preds = predict_edge(model, embeddings, edge_index)\n",
        "\n",
        "\n",
        "        if real_alter == 0:\n",
        "            y_true.append(0)\n",
        "            y_pred.append(0 if preds[0] < 0.5 else 1)\n",
        "        elif ambiguous_alter == 0:\n",
        "            y_true.append(1)\n",
        "            y_pred.append(1 if preds[0] >= 0.5 else 0)\n",
        "\n",
        "\n",
        "        results.append({\n",
        "            'pid': pid,\n",
        "            'real_alter': real_alter,\n",
        "            'ambiguous_alter': ambiguous_alter,\n",
        "            'prediction': preds[0],\n",
        "            'correct': y_pred[-1] == y_true[-1]\n",
        "        })\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "print(\"Results:\")\n",
        "print(results_df)\n",
        "print(\"\\nMetrics:\")\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "results_df"
      ],
      "metadata": {
        "id": "A1PlsBiSWKaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For Decision tree\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_probability(clf, features, source_node, target_node):\n",
        "\n",
        "    if source_node not in features.index or target_node not in features.index:\n",
        "        print(f\"Warning: Node {source_node} or {target_node} not found in features DataFrame.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    source_features = features.loc[source_node].values\n",
        "    target_features = features.loc[target_node].values\n",
        "\n",
        "\n",
        "    combined_features = np.concatenate((source_features, target_features))\n",
        "\n",
        "\n",
        "    combined_features = combined_features[:clf.n_features_in_].reshape(1, -1)\n",
        "\n",
        "\n",
        "    probability = clf.predict_proba(combined_features)[0, 1]\n",
        "    return probability\n",
        "\n",
        "\n",
        "df_ambiguous = ambig_res\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for _, row in df_ambiguous.iterrows():\n",
        "    pid = row['pid']\n",
        "    real_alter = row['real alter']\n",
        "    ambiguous_alter = row['ambiguous alter']\n",
        "\n",
        "\n",
        "    prob_real_alter = predict_probability(clf, features, pid, real_alter)\n",
        "    prob_ambiguous_alter = predict_probability(clf, features, pid, ambiguous_alter)\n",
        "\n",
        "\n",
        "    if prob_real_alter is None or prob_ambiguous_alter is None:\n",
        "        continue\n",
        "\n",
        "\n",
        "    y_true.append(1)\n",
        "    predicted_label = 1 if prob_real_alter > prob_ambiguous_alter else 0\n",
        "    y_pred.append(predicted_label)\n",
        "\n",
        "\n",
        "    results.append({\n",
        "        'pid': pid,\n",
        "        'real_alter': real_alter,\n",
        "        'ambiguous_alter': ambiguous_alter,\n",
        "        'probability_real_alter': prob_real_alter,\n",
        "        'probability_ambiguous_alter': prob_ambiguous_alter,\n",
        "        'predicted_closer': 'real_alter' if predicted_label == 1 else 'ambiguous_alter',\n",
        "        'true_closer': 'real_alter',\n",
        "        'correct': predicted_label == 1\n",
        "    })\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "\n",
        "\n",
        "print(\"Results:\")\n",
        "print(results_df)\n",
        "print(\"\\nMetrics:\")\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "\n",
        "for idx, result in results_df.iterrows():\n",
        "    print(f\"\\nPID: {result['pid']}, Real Alter: {result['real_alter']}, Ambiguous Alter: {result['ambiguous_alter']}\")\n",
        "    print(f\"Predicted Closer: {result['predicted_closer']}, True Closer: {result['true_closer']}\")\n",
        "    print(f\"Probability of Real Alter: {result['probability_real_alter']:.4f}, Probability of Ambiguous Alter: {result['probability_ambiguous_alter']:.4f}\")\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "hkSlUV8hui6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# ✅ Predict Probability Function\n",
        "def predict_probability(clf, features, source_node, target_node):\n",
        "    if source_node not in features.index or target_node not in features.index:\n",
        "        print(f\"Warning: Node {source_node} or {target_node} not found in features DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    source_features = features.loc[source_node].values\n",
        "    target_features = features.loc[target_node].values\n",
        "\n",
        "    combined_features = np.concatenate((source_features, target_features))\n",
        "    combined_features = combined_features[:clf.n_features_in_].reshape(1, -1)\n",
        "\n",
        "    probability = clf.predict_proba(combined_features)[0, 1]\n",
        "    return probability\n",
        "\n",
        "\n",
        "# ✅ Load Ambiguous Cases\n",
        "df_ambiguous = ambig_res\n",
        "results = []\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "\n",
        "# ✅ Iterate Over Ambiguous Pairs\n",
        "for _, row in df_ambiguous.iterrows():\n",
        "    pid = row['pid']\n",
        "    real_alter = row['real alter']\n",
        "    ambiguous_alter = row['ambiguous alter']\n",
        "\n",
        "    prob_real_alter = predict_probability(clf, features, pid, real_alter)\n",
        "    prob_ambiguous_alter = predict_probability(clf, features, pid, ambiguous_alter)\n",
        "\n",
        "    if prob_real_alter is None or prob_ambiguous_alter is None:\n",
        "        continue\n",
        "\n",
        "    y_true.append(1)\n",
        "    predicted_label = 1 if prob_real_alter > prob_ambiguous_alter else 0\n",
        "    y_pred.append(predicted_label)\n",
        "\n",
        "    results.append({\n",
        "        'pid': pid,\n",
        "        'real_alter': real_alter,\n",
        "        'ambiguous_alter': ambiguous_alter,\n",
        "        'probability_real_alter': prob_real_alter,\n",
        "        'probability_ambiguous_alter': prob_ambiguous_alter,\n",
        "        'predicted_closer': 'real_alter' if predicted_label == 1 else 'ambiguous_alter',\n",
        "        'true_closer': 'real_alter',\n",
        "        'correct': predicted_label == 1\n",
        "    })\n",
        "\n",
        "\n",
        "# ✅ Convert Results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# ✅ Compute Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "\n",
        "\n",
        "# ✅ Print Results\n",
        "print(\"Results:\")\n",
        "print(results_df)\n",
        "print(\"\\nMetrics:\")\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "\n",
        "# ✅ Print Detailed Per-Pair Results\n",
        "for idx, result in results_df.iterrows():\n",
        "    print(f\"\\nPID: {result['pid']}, Real Alter: {result['real_alter']}, Ambiguous Alter: {result['ambiguous_alter']}\")\n",
        "    print(f\"Predicted Closer: {result['predicted_closer']}, True Closer: {result['true_closer']}\")\n",
        "    print(f\"Probability of Real Alter: {result['probability_real_alter']:.4f}, Probability of Ambiguous Alter: {result['probability_ambiguous_alter']:.4f}\")\n",
        "    print(\"-----\")\n"
      ],
      "metadata": {
        "id": "3Q1HpFSjfjNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch.optim import AdamW\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import torch.nn.functional as F\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Define GraphSAGE model\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "        self.conv_out = SAGEConv(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv_out(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "    def decode_all(self, z, pos_edge_index, neg_edge_index):\n",
        "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1)\n",
        "        return torch.sigmoid(self.decode(z, edge_index))\n",
        "\n",
        "def train_model(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "\n",
        "    pos_edge_index = data.edge_index\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=data.edge_index,\n",
        "        num_nodes=data.num_nodes,\n",
        "        num_neg_samples=data.edge_index.size(1)\n",
        "    )\n",
        "\n",
        "    pos_pred = torch.sigmoid(model.decode(z, pos_edge_index))\n",
        "    neg_pred = torch.sigmoid(model.decode(z, neg_edge_index))\n",
        "\n",
        "    pos_labels = torch.ones(pos_pred.size(0), dtype=torch.float).to(device)\n",
        "    neg_labels = torch.zeros(neg_pred.size(0), dtype=torch.float).to(device)\n",
        "\n",
        "    pos_loss = -torch.log(pos_pred).mean()\n",
        "    neg_loss = -torch.log(1 - neg_pred).mean()\n",
        "    loss = pos_loss + neg_loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    all_preds = torch.cat([pos_pred, neg_pred])\n",
        "    all_labels = torch.cat([pos_labels, neg_labels])\n",
        "\n",
        "    all_preds_binary = (all_preds > 0.5).float()\n",
        "    accuracy = (all_preds_binary == all_labels).float().mean().item()\n",
        "\n",
        "    return loss.item(), accuracy, z\n",
        "\n",
        "def clear_directory(directory):\n",
        "    if os.path.exists(directory):\n",
        "        shutil.rmtree(directory)\n",
        "    os.makedirs(directory)\n",
        "\n",
        "def prepare_data_with_node_mapping(embeddings, edge_index):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for i in range(edge_index.size(1)):\n",
        "        src = edge_index[0, i]\n",
        "        dst = edge_index[1, i]\n",
        "        feature = np.concatenate((embeddings[src], embeddings[dst]))\n",
        "        label = 1  # assuming all edges in edge_index are positive samples\n",
        "        features.append(feature)\n",
        "        labels.append(label)\n",
        "    features = np.array(features)\n",
        "    labels = np.array(labels)\n",
        "    return features, labels\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_channels = 87\n",
        "lr = 0.0006319720394446876\n",
        "num_layers = 3\n",
        "dropout = 0.09011363149848563\n",
        "epochs = 6465\n",
        "weight_decay = 1.5091468730235952e-09\n",
        "out_channels = 7\n",
        "\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "\n",
        "x = torch.tensor(df_reindexed.values.astype(np.float32))\n",
        "edge_index = torch.tensor([df['pid'].values, df['alter'].values], dtype=torch.long)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "split = RandomLinkSplit(num_val=0.1, num_test=0.1)\n",
        "train_data, val_data, test_data = split(data)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_data_selected = Data(x=x, edge_index=train_data.edge_index)\n",
        "val_data_selected = Data(x=x, edge_index=val_data.edge_index)\n",
        "test_data_selected = Data(x=x, edge_index=test_data.edge_index)\n",
        "\n",
        "model = GraphSAGE(\n",
        "    in_channels=x.size(1),\n",
        "    hidden_channels=hidden_channels,\n",
        "    out_channels=out_channels,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss, accuracy, embeddings = train_model(model, train_data_selected, optimizer)\n",
        "    if epoch % 100 == 0:  # Print every 100 epochs\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save embeddings\n",
        "embeddings_save_dir = '/content/embeddings'\n",
        "clear_directory(embeddings_save_dir)\n",
        "\n",
        "embeddings_path = os.path.join(embeddings_save_dir, f\"embeddings.pt\")\n",
        "torch.save(embeddings, embeddings_path)\n",
        "print(f\"Embeddings saved to {embeddings_path}\")\n",
        "\n",
        "# Evaluation and Prediction\n",
        "model_save_path = '/content/graphsage_model.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "# Load the saved model and embeddings\n",
        "model = GraphSAGE(\n",
        "    in_channels=x.size(1),\n",
        "    hidden_channels=hidden_channels,\n",
        "    out_channels=out_channels,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "\n",
        "embeddings = torch.load(embeddings_path)\n",
        "\n",
        "def predict_edge(model, embeddings, edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = torch.sigmoid(model.decode_all(embeddings, edge_index, edge_index)).cpu().numpy()\n",
        "    return preds\n",
        "\n",
        "# Use the existing `ambig_res` DataFrame\n",
        "df_ambiguous = ambig_res  # Ensure `ambig_res` DataFrame is defined\n",
        "\n",
        "# Create DataFrame for results\n",
        "results = []\n",
        "\n",
        "# Collect ground truth and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for _, row in df_ambiguous.iterrows():\n",
        "    pid = row['pid']\n",
        "    real_alter = row['real alter']\n",
        "    ambiguous_alter = row['ambiguous alter']\n",
        "\n",
        "    if real_alter == 0 or ambiguous_alter == 0:\n",
        "        # Construct edge index for prediction\n",
        "        edge_index = torch.tensor([[real_alter, ambiguous_alter], [ambiguous_alter, real_alter]], dtype=torch.long).t().contiguous().to(device)\n",
        "\n",
        "        # Generate prediction\n",
        "        preds = predict_edge(model, embeddings, edge_index)\n",
        "\n",
        "        # Determine ground truth and prediction\n",
        "        if real_alter == 0:\n",
        "            y_true.append(0)  # Connection should not exist\n",
        "            y_pred.append(0 if preds[0] < 0.5 else 1)\n",
        "        elif ambiguous_alter == 0:\n",
        "            y_true.append(1)  # Connection should exist\n",
        "            y_pred.append(1 if preds[0] >= 0.5 else 0)\n",
        "\n",
        "        # Append result for printing\n",
        "        results.append({\n",
        "            'pid': pid,\n",
        "            'real_alter': real_alter,\n",
        "            'ambiguous_alter': ambiguous_alter,\n",
        "            'prediction': preds[0],\n",
        "            'correct': y_pred[-1] == y_true[-1]\n",
        "        })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Calculate accuracy metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "# Print results and metrics\n",
        "print(\"Results:\")\n",
        "print(results_df)\n",
        "print(\"\\nMetrics:\")\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "Sv-FJQJcslkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Explainer"
      ],
      "metadata": {
        "id": "3xRBdCEIIbn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric explainer\n",
        "!pip install captum"
      ],
      "metadata": {
        "id": "x2zcvCN8SJ_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(df_reindexed.values.astype(np.float32))\n",
        "edge_index = torch.tensor([df['pid'], df['alter']], dtype=torch.long)"
      ],
      "metadata": {
        "id": "tLkpEIJKaxqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainer for GAT"
      ],
      "metadata": {
        "id": "Mvd5DcX8vvg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.explain import Explainer, GNNExplainer, ModelConfig\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "class GraphGAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, heads):\n",
        "        super(GraphGAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "        self.conv_out = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv_out(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GraphGAT(in_channels=145, hidden_channels=16, out_channels=7, num_layers=1, dropout=0.062244324375886104, heads=8)\n",
        "model.load_state_dict(torch.load('graphgat_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "x = torch.tensor(df_reindexed.values.astype(np.float32)).to(device)\n",
        "edge_index = torch.tensor([df['pid'].values, df['alter'].values], dtype=torch.long).to(device)\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=model,\n",
        "    explanation_type='model',\n",
        "    algorithm=GNNExplainer(epochs=898),\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=ModelConfig(\n",
        "        mode='binary_classification',\n",
        "        task_level='edge',\n",
        "        return_type='raw',\n",
        "    )\n",
        ")\n",
        "def explain_link(source_node, dest_node, model, edge_index, data, explainer, feature_names):\n",
        "    # Convert the data to a NetworkX graph\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    sub_nodes = [source_node, dest_node] + list(G.neighbors(source_node)) + list(G.neighbors(dest_node))\n",
        "    sub_G = G.subgraph(sub_nodes).copy()\n",
        "\n",
        "    x = data.x.clone().detach().requires_grad_(True).to(device)\n",
        "    edge_index_for_prediction = torch.tensor([[source_node, dest_node], [dest_node, source_node]], dtype=torch.long).t().to(device)\n",
        "\n",
        "    # Get explanation using the explainer\n",
        "    explanation = explainer(\n",
        "        x=x,\n",
        "        edge_index=edge_index_for_prediction,\n",
        "        target=None\n",
        "    )\n",
        "    node_mask = explanation.node_mask\n",
        "    edge_mask = explanation.edge_mask\n",
        "\n",
        "    important_edges = edge_index_for_prediction[:, edge_mask > 0.5].cpu().numpy()\n",
        "    important_nodes = torch.nonzero(node_mask > 0.5).cpu().numpy().flatten()\n",
        "\n",
        "    # Plot the subgraph with highlighted important edges\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(sub_G)\n",
        "    nx.draw(sub_G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)\n",
        "    nx.draw_networkx_edges(sub_G, pos, edgelist=important_edges.T, edge_color='red', width=2)\n",
        "    plt.title(f'Subgraph around nodes {source_node} and {dest_node}')\n",
        "    plt.show()\n",
        "\n",
        "    print(f'\\nExplanation for link between nodes {source_node} and {dest_node}:')\n",
        "    print(f\"Important Edges: {important_edges}\")\n",
        "\n",
        "    # Find and print the most important features for both source and target nodes\n",
        "    source_important_features = []\n",
        "    dest_important_features = []\n",
        "\n",
        "    print(\"\\nTop important features for source and target nodes in the subgraph:\")\n",
        "    for node in [source_node, dest_node]:\n",
        "        if node in sub_nodes:\n",
        "            node_features = x[node].detach().cpu().numpy()\n",
        "            print(f\"Node {node}:\")\n",
        "            important_features = []\n",
        "            for idx in range(len(node_features)):\n",
        "                if node_features[idx] == 1:  # Adjust the condition based on how importance is determined\n",
        "                    feature_name = feature_names[idx]\n",
        "                    feature_value = node_features[idx]\n",
        "                    important_features.append(feature_name)\n",
        "                    print(f\"  {feature_name}: {feature_value}\")\n",
        "\n",
        "            if node == source_node:\n",
        "                source_important_features = important_features\n",
        "            elif node == dest_node:\n",
        "                dest_important_features = important_features\n",
        "\n",
        "    # Identify common important features\n",
        "    common_features = list(set(source_important_features) & set(dest_important_features))\n",
        "\n",
        "    print(\"\\nCommon important features for both source and destination nodes:\")\n",
        "    if common_features:\n",
        "        for feature in common_features:\n",
        "            print(f\"  {feature}\")\n",
        "    else:\n",
        "        print(\"  None\")\n",
        "\n",
        "    # Find shared neighbors that may influence connection\n",
        "    source_neighbors = set(G.neighbors(source_node))\n",
        "    dest_neighbors = set(G.neighbors(dest_node))\n",
        "\n",
        "    # Find common neighbors between source and target nodes\n",
        "    shared_neighbors = source_neighbors & dest_neighbors\n",
        "\n",
        "    print(\"\\nShared neighbors that might influence the connection:\")\n",
        "    if shared_neighbors:\n",
        "        for shared_node in shared_neighbors:\n",
        "            print(f\"  Node {shared_node} is a shared neighbor between source {source_node} and destination {dest_node}.\")\n",
        "    else:\n",
        "        print(\"  No shared neighbors found.\")\n",
        "\n",
        "    return edge_mask, node_mask, sub_G\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(27)\n",
        "random_pairs = [(edge_index[0, i].item(), edge_index[1, i].item()) for i in np.random.choice(edge_index.shape[1], 20, replace=False)]\n",
        "\n",
        "\n",
        "feature_names = data_features_2.columns.tolist()\n",
        "\n",
        "\n",
        "for source_node, dest_node in random_pairs:\n",
        "    edge_mask, node_mask, sub_G = explain_link(source_node, dest_node, model, edge_index, data, explainer, feature_names)\n"
      ],
      "metadata": {
        "id": "99Vd-_y9vuqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.explain import Explainer, GNNExplainer, ModelConfig\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "class GraphGAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, heads):\n",
        "        super(GraphGAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "        self.conv_out = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv_out(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.conv_out(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GraphGAT(in_channels=145, hidden_channels=16, out_channels=7, num_layers=1, dropout=0.062244324375886104, heads=8)\n",
        "model.load_state_dict(torch.load('graphgat_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=model,\n",
        "    explanation_type='model',\n",
        "    algorithm=GNNExplainer(epochs=898),\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=ModelConfig(\n",
        "        mode='binary_classification',\n",
        "        task_level='edge',\n",
        "        return_type='raw',\n",
        "    )\n",
        ")\n",
        "def explain_link(source_node, dest_node, model, edge_index, data, explainer, feature_names):\n",
        "    # Convert the data to a NetworkX graph\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    sub_nodes = [source_node, dest_node] + list(G.neighbors(source_node)) + list(G.neighbors(dest_node))\n",
        "    sub_G = G.subgraph(sub_nodes).copy()\n",
        "\n",
        "    x = data.x.clone().detach().requires_grad_(True).to(device)\n",
        "    edge_index_for_prediction = torch.tensor([[source_node, dest_node], [dest_node, source_node]], dtype=torch.long).t().to(device)\n",
        "\n",
        "    # Get explanation using the explainer\n",
        "    explanation = explainer(\n",
        "        x=x,\n",
        "        edge_index=edge_index_for_prediction,\n",
        "        target=None\n",
        "    )\n",
        "    node_mask = explanation.node_mask\n",
        "    edge_mask = explanation.edge_mask\n",
        "\n",
        "    important_edges = edge_index_for_prediction[:, edge_mask > 0.5].cpu().numpy()\n",
        "    important_nodes = torch.nonzero(node_mask > 0.5).cpu().numpy().flatten()\n",
        "\n",
        "    # Plot the subgraph with highlighted important edges\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(sub_G)\n",
        "    nx.draw(sub_G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)\n",
        "    nx.draw_networkx_edges(sub_G, pos, edgelist=important_edges.T, edge_color='red', width=2)\n",
        "    plt.title(f'Subgraph around nodes {source_node} and {dest_node}')\n",
        "    plt.show()\n",
        "\n",
        "    print(f'\\nExplanation for link between nodes {source_node} and {dest_node}:')\n",
        "    print(f\"Important Edges: {important_edges}\")\n",
        "\n",
        "    # Find and print the most important features for both source and target nodes\n",
        "    source_important_features = []\n",
        "    dest_important_features = []\n",
        "\n",
        "    print(\"\\nTop important features for source and target nodes in the subgraph:\")\n",
        "    for node in [source_node, dest_node]:\n",
        "        if node in sub_nodes:\n",
        "            node_features = x[node].detach().cpu().numpy()\n",
        "            print(f\"Node {node}:\")\n",
        "            important_features = []\n",
        "            for idx in range(len(node_features)):\n",
        "                if node_features[idx] == 1:  # Adjust the condition based on how importance is determined\n",
        "                    feature_name = feature_names[idx]\n",
        "                    feature_value = node_features[idx]\n",
        "                    important_features.append(feature_name)\n",
        "                    print(f\"  {feature_name}: {feature_value}\")\n",
        "\n",
        "            if node == source_node:\n",
        "                source_important_features = important_features\n",
        "            elif node == dest_node:\n",
        "                dest_important_features = important_features\n",
        "\n",
        "    # Identify common important features\n",
        "    common_features = list(set(source_important_features) & set(dest_important_features))\n",
        "\n",
        "    print(\"\\nCommon important features for both source and destination nodes:\")\n",
        "    if common_features:\n",
        "        for feature in common_features:\n",
        "            print(f\"  {feature}\")\n",
        "    else:\n",
        "        print(\"  None\")\n",
        "\n",
        "    return edge_mask, node_mask, sub_G\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "random_pairs = ()#[(edge_index[0, i].item(), edge_index[1, i].item()) for i in np.random.choice(edge_index.shape[1], 10, replace=False)]\n",
        "\n",
        "pairs = []\n",
        "for _, row in ambig_res_filtered.iterrows():\n",
        "    pairs.append((row['pid'], row['real alter']))       # Pair: pid -> real_alter\n",
        "    pairs.append((row['pid'], row['ambiguous alter']))   # Pair: pid -> ambiguous_alter\n",
        "\n",
        "print(pairs)\n",
        "\n",
        "feature_names = data_features_2.columns.tolist()\n",
        "\n",
        "\n",
        "for source_node, dest_node in pairs:\n",
        "    edge_mask, node_mask, sub_G = explain_link(source_node, dest_node, model, edge_index, data, explainer, feature_names)\n"
      ],
      "metadata": {
        "id": "AQEC82VpLIxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Suicide Score Prediction"
      ],
      "metadata": {
        "id": "7lyseTWXdJ0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_final_2 = data_2.drop(columns=['suic_1', 'suic_2', 'suic_3', 'suic_4'])\n",
        "\n",
        "X = data_final_2.drop(columns=['pid', 'suicidal_score'])\n",
        "y = data_final_2['suicidal_score']\n",
        "\n",
        "\n",
        "X_cleaned = X.dropna(axis=1, how='all')\n",
        "X_filled = X_cleaned.fillna(0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_filled, y, test_size=0.25)\n",
        "\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "\n",
        "X_train_oversampled, y_train_oversampled = resample(\n",
        "    X_train, y_train,\n",
        "    replace=True,\n",
        "    n_samples=int(2.5 * len(X_train))\n",
        ")\n",
        "\n",
        "\n",
        "noise = np.random.normal(0, 0.01, X_train_oversampled.shape)\n",
        "X_train_oversampled = X_train_oversampled + noise\n",
        "\n",
        "X_combined = pd.concat([X_train_oversampled, X_test]).reset_index(drop=True)\n",
        "y_combined = pd.concat([y_train_oversampled, y_test]).reset_index(drop=True)\n",
        "\n",
        "x = torch.tensor(X_combined.values.astype(np.float32))\n",
        "y = torch.tensor(y_combined.values.astype(np.float32))\n",
        "\n",
        "train_mask = torch.zeros(x.size(0), dtype=torch.bool)\n",
        "test_mask = torch.zeros(x.size(0), dtype=torch.bool)\n",
        "train_mask[:len(X_train_oversampled)] = True\n",
        "test_mask[len(X_train_oversampled):] = True\n",
        "\n",
        "#df_edges = pd.read_csv('/confident_edgelist2.csv')\n",
        "df_edges = pd.read_csv('/updated_edge_list.csv')\n",
        "edge_index = torch.tensor([df_edges['pid'].values, df_edges['alter'].values], dtype=torch.long)\n",
        "\n",
        "num_nodes = x.size(0)\n",
        "valid_mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)\n",
        "edge_index = edge_index[:, valid_mask]\n",
        "\n",
        "train_nodes = torch.where(train_mask)[0]\n",
        "test_nodes = torch.where(test_mask)[0]\n",
        "valid_edges_mask = ~(\n",
        "    (torch.isin(edge_index[0], train_nodes) & torch.isin(edge_index[1], test_nodes)) |\n",
        "    (torch.isin(edge_index[0], test_nodes) & torch.isin(edge_index[1], train_nodes))\n",
        ")\n",
        "edge_index = edge_index[:, valid_edges_mask]\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "print(f\"Data object created with {data.x.size(0)} nodes and {data.edge_index.size(1)} edges.\")\n"
      ],
      "metadata": {
        "id": "UtjpFTTTVDzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "7DFLp_xoeGRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "regressor = DecisionTreeRegressor()\n",
        "\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared Score: {r2:.4f}\")\n",
        "\n",
        "threshold = 7\n",
        "\n",
        "y_pred_binary = (y_pred > threshold).astype(int)\n",
        "y_test_binary = (y_test > threshold).astype(int)\n",
        "\n",
        "auc = roc_auc_score(y_test_binary, y_pred_binary)\n",
        "\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Correlation')\n",
        "plt.title('True Suicidal Score vs Predicted Suicidal Score (Decision Tree)')\n",
        "plt.xlabel('True Suicidal Score')\n",
        "plt.ylabel('Predicted Suicidal Score')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lOW62MqjWmJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "ae5MpwUIeO8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
        "mlp_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mlp_regressor.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "\n",
        "y_pred_binary = (y_pred > threshold).astype(int)\n",
        "y_test_binary = (y_test > threshold).astype(int)\n",
        "\n",
        "auc = roc_auc_score(y_test_binary, y_pred_binary)\n",
        "\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Correlation')\n",
        "plt.title('True Suicidal Score vs Predicted Suicidal Score (MLP Regressor)')\n",
        "plt.xlabel('True Suicidal Score')\n",
        "plt.ylabel('Predicted Suicidal Score')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jAv1XXFCXJZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAT"
      ],
      "metadata": {
        "id": "sbD9MW-kdOaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, roc_curve, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load features and calculate raw suicidal scores\n",
        "data_file = '/main_data.csv'\n",
        "features = pd.read_csv(data_file)\n",
        "features[['suic_1', 'suic_2', 'suic_3', 'suic_4']] = features[['suic_1', 'suic_2', 'suic_3', 'suic_4']].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill NaN values with 0\n",
        "features = features.fillna(0)\n",
        "\n",
        "# Calculate the raw suicidal score\n",
        "features['raw_suicidal_score'] = (\n",
        "    features['suic_1'].fillna(0) +\n",
        "    features['suic_2'].fillna(0) +\n",
        "    features['suic_3'].fillna(0) +\n",
        "    features['suic_4'].fillna(0)\n",
        ")\n",
        "\n",
        "# Drop individual suicide columns\n",
        "features = features.drop(columns=['suic_1', 'suic_2', 'suic_3', 'suic_4'])\n",
        "\n",
        "# Check if 'pid' column exists\n",
        "if 'pid' not in features.columns:\n",
        "    raise KeyError(\"The 'pid' column is missing from the features DataFrame.\")\n",
        "\n",
        "# Step 1: Oversample based on raw scores\n",
        "high_scores = features[features['raw_suicidal_score'] > 7]\n",
        "low_scores = features[features['raw_suicidal_score'] <= 7]\n",
        "\n",
        "# Perform oversampling\n",
        "oversampling_factor = max(1, int(len(low_scores) / len(high_scores)))  # Determine oversampling factor\n",
        "high_scores_oversampled = resample(\n",
        "    high_scores,\n",
        "    replace=True,\n",
        "    n_samples=oversampling_factor * len(high_scores),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Combine oversampled high-risk samples with low-risk samples\n",
        "combined_features = pd.concat([low_scores, high_scores_oversampled], ignore_index=True)\n",
        "\n",
        "# Step 2: Apply sigmoid scaling after oversampling\n",
        "combined_features['suicidal_score'] = 1 / (1 + np.exp(-combined_features['raw_suicidal_score']))\n",
        "\n",
        "# Calculate the sigmoid-scaled threshold for a raw score of 7\n",
        "threshold_scaled = 1 / (1 + np.exp(-7))\n",
        "\n",
        "# Drop the raw suicidal score after scaling\n",
        "combined_features = combined_features.drop(columns=['raw_suicidal_score'])\n",
        "\n",
        "# Step 3: Split into above and below threshold groups\n",
        "above_threshold = combined_features[combined_features['suicidal_score'] > threshold_scaled]\n",
        "below_threshold = combined_features[combined_features['suicidal_score'] <= threshold_scaled]\n",
        "\n",
        "# Train-test split for each group\n",
        "above_train, above_test = train_test_split(above_threshold, test_size=0.15)\n",
        "below_train, below_test = train_test_split(below_threshold, test_size=0.15)\n",
        "\n",
        "# Balance the training samples\n",
        "min_train_samples = min(len(above_train), len(below_train))\n",
        "above_train_balanced = resample(above_train, replace=False, n_samples=min_train_samples)\n",
        "below_train_balanced = resample(below_train, replace=False, n_samples=min_train_samples)\n",
        "\n",
        "# Balance the testing samples\n",
        "min_test_samples = min(len(above_test), len(below_test))\n",
        "above_test_balanced = resample(above_test, replace=False, n_samples=min_test_samples)\n",
        "below_test_balanced = resample(below_test, replace=False, n_samples=min_test_samples)\n",
        "\n",
        "# Combine balanced samples for training and testing\n",
        "train_df = pd.concat([above_train_balanced, below_train_balanced], ignore_index=True)\n",
        "test_df = pd.concat([above_test_balanced, below_test_balanced], ignore_index=True)\n",
        "\n",
        "# Print training and testing sample distribution\n",
        "def print_sample_distribution(df, name, threshold_scaled):\n",
        "    below_threshold = df[df['suicidal_score'] <= threshold_scaled].shape[0]\n",
        "    above_threshold = df[df['suicidal_score'] > threshold_scaled].shape[0]\n",
        "    print(f\"{name} Samples Below Threshold ({threshold_scaled:.4f}): {below_threshold}\")\n",
        "    print(f\"{name} Samples Above Threshold ({threshold_scaled:.4f}): {above_threshold}\")\n",
        "\n",
        "print_sample_distribution(train_df, \"Training\", threshold_scaled)\n",
        "print_sample_distribution(test_df, \"Testing\", threshold_scaled)\n",
        "\n",
        "# Step 4: Prepare tensors for training and testing\n",
        "X_train = train_df.drop(columns=['suicidal_score', 'pid'])\n",
        "y_train = train_df['suicidal_score']\n",
        "X_test = test_df.drop(columns=['suicidal_score', 'pid'])\n",
        "y_test = test_df['suicidal_score']\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = torch.tensor(X_train.values.astype(np.float32)).to(device)\n",
        "y = torch.tensor(y_train.values.astype(np.float32)).to(device)\n",
        "x_test = torch.tensor(X_test.values.astype(np.float32)).to(device)\n",
        "y_test = torch.tensor(y_test.values.astype(np.float32)).to(device)\n",
        "\n",
        "# Define GAT Regressive Model\n",
        "class GATRegressor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, dropout, heads):\n",
        "        super(GATRegressor, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout))\n",
        "        self.conv_out = GATConv(hidden_channels * heads, 1, heads=1, concat=False, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index).relu()\n",
        "            x = F.dropout(x, p=0.06631633790503316, training=self.training)\n",
        "        return self.conv_out(x, edge_index)\n",
        "\n",
        "x = torch.nan_to_num(x, nan=0.0)  # Replace NaNs in the feature tensor with 0\n",
        "y = torch.nan_to_num(y, nan=0.0)\n",
        "\n",
        "# Remaining parts of the code (AUC evaluation, edge list processing) remain the same\n",
        "\n",
        "\n",
        "# Evaluate edge list and plot AUC curve\n",
        "def evaluate_edge_list_with_auc(edge_file, label):\n",
        "    # Load edge list\n",
        "    edge_list = pd.read_csv(edge_file)\n",
        "\n",
        "    # Filter edges based on valid PIDs\n",
        "    valid_edges = edge_list[\n",
        "        edge_list['pid'].isin(combined_features['pid']) &\n",
        "        edge_list['alter'].isin(combined_features['pid'])\n",
        "    ]\n",
        "    edge_index = torch.tensor([valid_edges['pid'].values, valid_edges['alter'].values], dtype=torch.long)\n",
        "\n",
        "    # Create the data object\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = GATRegressor(\n",
        "        in_channels=data.x.size(1),\n",
        "        hidden_channels=26,\n",
        "        num_layers=2,\n",
        "        dropout=0.06631633790503316,\n",
        "        heads=10\n",
        "    ).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=0.006405718308667949, weight_decay=0.00014253605892059957)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(80):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index).squeeze()\n",
        "        loss = F.mse_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate the model\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        regressed_scores = model(data.x, data.edge_index).squeeze().cpu().numpy()\n",
        "\n",
        "        # Calculate MSE, MAE, and AUC\n",
        "        mse = mean_squared_error(y.cpu().numpy(), regressed_scores)\n",
        "        mae = mean_absolute_error(y.cpu().numpy(), regressed_scores)\n",
        "        auc = roc_auc_score((y.cpu() > threshold_scaled).to(torch.int).numpy(), regressed_scores)\n",
        "\n",
        "        # Plot AUC curve\n",
        "        fpr, tpr, _ = roc_curve((y.cpu() > threshold_scaled).to(torch.int).numpy(), regressed_scores)\n",
        "        RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
        "        plt.title(f\"AUC Curve - {label}\")\n",
        "        plt.show()\n",
        "\n",
        "    return mse, mae, auc\n",
        "\n",
        "\n",
        "# Evaluate `confident_edgelist2` (before disambiguation)\n",
        "mse_confident, mae_confident, auc_confident = evaluate_edge_list_with_auc('/confident_edgelist2.csv', \"Confident Edgelist 2\")\n",
        "\n",
        "# Evaluate `updated_edge_list` (after disambiguation)\n",
        "mse_updated, mae_updated, auc_updated = evaluate_edge_list_with_auc('/updated_edge_list.csv', \"Updated Edgelist\")\n",
        "\n",
        "# Output the results\n",
        "print(\"Results for confident_edgelist2 (before disambiguation):\")\n",
        "print(f\"MSE: {mse_confident:.4f}, MAE: {mae_confident:.4f}, AUC: {auc_confident:.4f}\")\n",
        "print(\"\\nResults for updated_edge_list (after disambiguation):\")\n",
        "print(f\"MSE: {mse_updated:.4f}, MAE: {mae_updated:.4f}, AUC: {auc_updated:.4f}\")\n"
      ],
      "metadata": {
        "id": "IgxtnKAJHt_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}